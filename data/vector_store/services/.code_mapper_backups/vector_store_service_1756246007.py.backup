"""
Vector Store Service - Unified facade for vector store operations.

This module provides the VectorStoreService class, which acts as a facade
combining CRUD operations and search/filtering capabilities.

Features:
- Unified interface for all vector store operations
- CRUD operations via RedisMetadataCRUDService
- Search and filtering via RedisMetadataFilterService
- Simplified initialization and dependency management
- Backward compatibility with existing code

Usage:
    service = VectorStoreService(
        redis_client=redis_client,
        faiss_service=faiss_service,
        embedding_service=embedding_service
    )
    
    # CRUD operations
    await service.upsert_chunk(chunk)
    chunk_data = await service.get_chunk(uuid)
    await service.delete_chunk(uuid)
    
    # Search operations
    results = await service.search(chunk_query, limit=10)
    results = await service.search_by_text("machine learning", limit=10)
    results = await service.search_by_vector(vector, limit=10)
"""

import logging
import json
from typing import Dict, List, Any, Optional, Union, Tuple
from chunk_metadata_adapter import SemanticChunk
from chunk_metadata_adapter.chunk_query import ChunkQuery
import numpy as np

from vector_store.services.redis_metadata_crud import RedisMetadataCRUDService
# Import moved to avoid circular import - will be imported in __init__ method
from vector_store.services.faiss_index_service import FaissIndexService
from vector_store.services.vector_store_maintenance_service import VectorStoreMaintenanceService
from vector_store.services.bm25_search_service import BM25SearchService, create_bm25_service
from vector_store.exceptions import (
    RedisOperationError, ChunkValidationError, FaissSearchError,
    ServiceInitializationError, VectorStoreServiceError, VectorStoreSearchError,
    VectorStoreHybridSearchError, SearchError, UnexpectedError,
    EmbeddingGenerationError, BM25SearchError
)

logger = logging.getLogger("vector_store.vector_store_service")


class VectorStoreService:
    """
    Unified facade service for vector store operations.
    
    Combines CRUD operations and search/filtering capabilities into a single
    interface for easier usage and dependency management.
    
    Features:
    - CRUD operations: create, read, update, delete chunks
    - Search operations: text search, vector search, metadata filtering
    - Combined search: intersection of vector and metadata results
    - Batch operations support
    - Soft delete handling
    - Pagination and sorting
    
    Architecture:
    - RedisMetadataCRUDService: Handles all CRUD operations
    - RedisMetadataFilterService: Handles all search and filtering
    - FaissIndexService: Vector operations (injected into both services)
    - EmbeddingService: Text vectorization (optional)
    
    Usage:
        service = VectorStoreService(
            redis_client=redis_client,
            faiss_service=faiss_service,
            embedding_service=embedding_service
        )
        
        # CRUD
        uuid = await service.upsert_chunk(chunk)
        chunk_data = await service.get_chunk(uuid)
        await service.delete_chunk(uuid)
        
        # Search
        results = await service.search(chunk_query, limit=10)
        results = await service.search_by_text("query", limit=10)
    """

    def __init__(self,
                 redis_client,
                 faiss_service: FaissIndexService,
                 embedding_service=None,
                 redis_url: Optional[str] = None,
                 bm25_config: Optional[Dict[str, Any]] = None,
                 index_manager=None):
        """
        Initialize vector store service with required components.

        Args:
            redis_client: Redis client for metadata operations
            faiss_service: FAISS service for vector operations
            embedding_service: Service for text vectorization (optional)
            redis_url: Redis connection URL (alternative to redis_client)
            bm25_config: Configuration for BM25 search service (optional)

        Raises:
            ServiceInitializationError: If initialization fails
        """
        if not redis_client and not redis_url:
            raise ServiceInitializationError("VectorStoreService", "Either redis_client or redis_url must be provided")
        if not faiss_service:
            raise ServiceInitializationError("VectorStoreService", "FAISS service is required")
        
        # Initialize CRUD service
        self.crud_service = RedisMetadataCRUDService(
            redis_client=redis_client,
            redis_url=redis_url,
            faiss_service=faiss_service
        )
        
        # Initialize filter service with CRUD service as dependency
        # Import here to avoid circular import
        from vector_store.services.redis_metadata_filter import RedisMetadataFilterService
        self.filter_service = RedisMetadataFilterService(
            redis_client=redis_client or self.crud_service.redis,
            faiss_service=faiss_service,
            embedding_service=embedding_service,
            crud_service=self.crud_service,
            vector_store_service=self
        )
        
        # Initialize BM25 search service
        self.bm25_service = None
        if bm25_config:
            try:
                self.bm25_service = create_bm25_service(bm25_config)
                logger.info("BM25 search service initialized")
            except BM25SearchError as e:
                logger.warning(f"Failed to initialize BM25 service: {e}")
            except Exception as e:
                logger.warning(f"Unexpected error initializing BM25 service: {e}")
        
        # Store references for convenience
        self.redis = self.crud_service.redis
        self.faiss_service = faiss_service
        self.embedding_service = embedding_service
        
        # Store service references for backward compatibility
        self._crud_service = self.crud_service
        self._filter_service = self.filter_service
        self._maintenance_service = VectorStoreMaintenanceService(
            crud_service=self.crud_service,
            faiss_service=self.faiss_service,
            redis_client=self.redis
        )
        
        logger.info("VectorStoreService initialized successfully")

    # ============================================================================
    # CRUD OPERATIONS (delegated to RedisMetadataCRUDService)
    # ============================================================================

    async def upsert_chunk(self, chunk: SemanticChunk) -> str:
        """
        Create or update single chunk.
        
        Args:
            chunk: SemanticChunk object
            
        Returns:
            UUID of the upserted chunk
            
        Raises:
            ChunkValidationError: If chunk validation fails
            RedisOperationError: If Redis operation fails
        """
        # Upsert chunk to Redis and FAISS
        uuid = await self.crud_service.upsert_chunk(chunk)
        
        # Index for BM25 search if service available
        if self.bm25_service:
            try:
                await self.index_chunk_for_bm25(chunk)
            except BM25SearchError as e:
                logger.warning(f"Failed to index chunk {uuid} for BM25: {e}")
            except Exception as e:
                logger.warning(f"Unexpected error indexing chunk {uuid} for BM25: {e}")
        
        return uuid

    async def upsert_chunks(self, chunks: List[SemanticChunk]) -> List[str]:
        """
        Create or update multiple chunks.
        
        Args:
            chunks: List of SemanticChunk objects
            
        Returns:
            List of UUIDs of successfully upserted chunks
            
        Raises:
            ChunkValidationError: If chunk validation fails
            RedisOperationError: If Redis operation fails
        """
        # Upsert chunks to Redis and FAISS
        uuids = await self.crud_service.upsert_chunks(chunks)
        
        # Index for BM25 search if service available
        if self.bm25_service:
            for chunk in chunks:
                try:
                    await self.index_chunk_for_bm25(chunk)
                except BM25SearchError as e:
                    logger.warning(f"Failed to index chunk {chunk.uuid} for BM25: {e}")
                except Exception as e:
                    logger.warning(f"Unexpected error indexing chunk {chunk.uuid} for BM25: {e}")
        
        return uuids

    async def get_chunk(self, uuid: str, include_vectors: bool = True) -> Optional[Dict[str, Any]]:
        """
        Get single chunk by UUID with optional vector restoration from FAISS.
        
        Args:
            uuid: Chunk UUID
            include_vectors: Whether to include vectors from FAISS
            
        Returns:
            Chunk dictionary with vectors or None if not found
            
        Raises:
            RedisOperationError: If Redis operation fails
        """
        chunk_data = await self.crud_service.get_chunk(uuid)
        if chunk_data and include_vectors:
            await self._restore_vectors_from_redis([chunk_data])
        return chunk_data

    async def get_chunks(self, uuids: List[str], include_vectors: bool = True) -> List[Dict[str, Any]]:
        """
        Get chunks by UUIDs with optional vector restoration from FAISS.
        
        Args:
            uuids: List of chunk UUIDs
            include_vectors: Whether to include vectors from FAISS
            
        Returns:
            List of chunk dictionaries with vectors (None if not found)
            
        Raises:
            RedisOperationError: If Redis operation fails
        """
        chunks_data = await self.crud_service.get_chunks(uuids)
        if include_vectors:
            await self._restore_vectors_from_redis(chunks_data)
        return chunks_data

    async def delete_chunk(self, uuid: str) -> bool:
        """
        Soft delete single chunk by UUID.
        
        Args:
            uuid: Chunk UUID
            
        Returns:
            True if soft deleted successfully
            
        Raises:
            RedisOperationError: If Redis operation fails
        """
        return await self.crud_service.delete_chunk(uuid)

    async def delete_chunks(self, uuids: List[str]) -> List[str]:
        """
        Soft delete chunks by UUIDs.
        
        Args:
            uuids: List of chunk UUIDs to soft delete
            
        Returns:
            List of successfully soft deleted UUIDs
            
        Raises:
            RedisOperationError: If Redis operation fails
        """
        return await self.crud_service.delete_chunks(uuids)

    async def delete_records(self, metadata_filter: Optional[Dict[str, Any]] = None) -> int:
        """
        Delete records by metadata filter.
        
        Args:
            metadata_filter: Metadata filter for deletion, None for all records
            
        Returns:
            Number of deleted records
            
        Raises:
            RedisOperationError: If Redis operation fails
        """
        try:
            if metadata_filter is None:
                # Delete all records
                all_uuids = await self.crud_service.get_all_record_ids()
                if not all_uuids:
                    return 0
                deleted_uuids = await self.crud_service.delete_chunks(all_uuids)
                return len(deleted_uuids)
            else:
                # Delete by filter
                chunk_query = ChunkQuery(metadata=metadata_filter)
                matching_records = await self.filter_service.search(
                    chunk_query=chunk_query,
                    include_deleted=False
                )
                if not matching_records:
                    return 0
                
                uuids_to_delete = [record["uuid"] for record in matching_records]
                deleted_uuids = await self.crud_service.delete_chunks(uuids_to_delete)
                return len(deleted_uuids)
                
        except Exception as e:
            logger.error(f"Failed to delete records: {e}")
            raise

    async def hard_delete_records(self, uuids: List[str]) -> Tuple[int, List[str]]:
        """
        Hard delete records by UUIDs (physically remove from FAISS and Redis).
        
        Args:
            uuids: List of UUIDs to hard delete
            
        Returns:
            Tuple of (deleted_count, deleted_uuids)
            
        Raises:
            RedisOperationError: If Redis operation fails
            FaissOperationError: If FAISS operation fails
        """
        try:
            if not uuids:
                return 0, []
            
            # Use CRUD service for hard delete
            deleted_uuids = await self.crud_service.hard_delete_chunks(uuids)
            
            return len(deleted_uuids), deleted_uuids
                
        except Exception as e:
            logger.error(f"Failed to hard delete records: {e}")
            raise

    async def force_delete_by_uuids(self, uuids: List[str], force: bool = False) -> Dict[str, Any]:
        """
        Force delete chunks by UUIDs with bypass of restrictions.
        
        Args:
            uuids: List of UUIDs to force delete
            force: Force flag to bypass restrictions (must be True)
            
        Returns:
            Dict with deletion results including deleted, not_found, errors counts
            
        Raises:
            ValueError: If force is not True or UUIDs list is empty
            RedisOperationError: If Redis operation fails
        """
        if not force:
            raise ValueError("Force delete requires force=True")
        
        if not uuids:
            raise ValueError("UUIDs list cannot be empty")
        
        # Validate UUIDs format
        for uuid in uuids:
            if not isinstance(uuid, str) or len(uuid) == 0:
                raise ValueError(f"Invalid UUID format: {uuid}")
        
        deleted_count = 0
        not_found_count = 0
        errors_count = 0
        errors_uuids = []
        deleted_uuids = []
        
        for uuid in uuids:
            try:
                # Check if chunk exists
                chunk_data = await self.crud_service.get_chunk(uuid)
                if not chunk_data or len(chunk_data) == 0:
                    not_found_count += 1
                    continue
                
                # Force delete from database
                await self.crud_service.hard_delete_chunks([uuid])
                deleted_count += 1
                deleted_uuids.append(uuid)
                
            except Exception as e:
                logger.error(f"Failed to force delete chunk {uuid}: {e}")
                errors_count += 1
                errors_uuids.append(uuid)
        
        return {
            "deleted": deleted_count,
            "not_found": not_found_count,
            "errors": errors_count,
            "errors_uuids": errors_uuids,
            "deleted_uuids": deleted_uuids
        }

    async def chunk_deferred_cleanup(self, dry_run: bool = False, batch_size: int = 100) -> Dict[str, Any]:
        """
        Clean up deferred chunks (soft-deleted records).
        
        Args:
            dry_run: Run without actual deletion
            batch_size: Number of chunks to process per batch
            
        Returns:
            Dict with cleanup results
            
        Raises:
            ValueError: If batch_size is invalid
            RedisOperationError: If Redis operation fails
        """
        if batch_size < 1 or batch_size > 1000:
            raise ValueError("Batch size must be between 1 and 1000")
        
        # Get soft-deleted UUIDs
        deleted_uuids = await self.get_deleted_uuids()
        
        if not deleted_uuids:
            return {
                "cleaned_count": 0,
                "total_processed": 0,
                "dry_run": dry_run,
                "message": "No soft-deleted records found"
            }
        
        # Process in batches
        cleaned_count = 0
        total_processed = 0
        
        for i in range(0, len(deleted_uuids), batch_size):
            batch_uuids = deleted_uuids[i:i + batch_size]
            
            if not dry_run:
                # Actually delete chunks
                try:
                    deleted_batch = await self.crud_service.hard_delete_chunks(batch_uuids)
                    cleaned_count += len(deleted_batch)
                except Exception as e:
                    logger.error(f"Failed to delete batch of deferred chunks: {e}")
            else:
                # Just count what would be deleted
                cleaned_count += len(batch_uuids)
            
            total_processed += len(batch_uuids)
        
        message = f"Successfully cleaned up {cleaned_count} soft-deleted records" if cleaned_count > 0 else "No soft-deleted records found"
        
        return {
            "cleaned_count": cleaned_count,
            "total_processed": total_processed,
            "dry_run": dry_run,
            "message": message
        }

    async def chunk_hard_delete(self, uuids: List[str], confirm: bool = False) -> Dict[str, Any]:
        """
        Hard delete chunks from database with confirmation.
        
        Args:
            uuids: List of UUIDs to delete
            confirm: Confirmation flag for safety
            
        Returns:
            Dict with deletion results
            
        Raises:
            ValueError: If confirm is not True or UUIDs list is empty
            RedisOperationError: If Redis operation fails
        """
        if not confirm:
            raise ValueError("Hard delete requires confirm=True")
        
        if not uuids:
            raise ValueError("UUIDs list cannot be empty")
        
        # Validate UUIDs format
        for uuid in uuids:
            if not isinstance(uuid, str) or len(uuid) == 0:
                raise ValueError(f"Invalid UUID format: {uuid}")
        
        # Perform hard delete
        deleted_count, deleted_uuids = await self.hard_delete_records(uuids)
        
        return {
            "deleted_count": deleted_count,
            "deleted_uuids": deleted_uuids,
            "total_requested": len(uuids)
        }

    async def get_all_record_ids(self, uuids: List[str] = None) -> List[str]:
        """
        Get chunk UUIDs from Redis.
        
        Args:
            uuids: Optional list of UUIDs to filter by. If None, returns all UUIDs.
            
        Returns:
            List of chunk UUIDs (filtered by uuids if provided)
            
        Raises:
            RedisOperationError: If Redis operation fails
        """
        return await self.crud_service.get_all_record_ids(uuids)

    async def count_records(self) -> int:
        """
        Count total number of chunks in Redis.
        
        Returns:
            Number of chunks
            
        Raises:
            RedisOperationError: If Redis operation fails
        """
        return await self.crud_service.count_records()

    async def verify_data_consistency(self) -> Dict[str, Any]:
        """
        Verify consistency between Redis and FAISS data.
        
        Returns:
            Dictionary with consistency information
            
        Raises:
            RedisOperationError: If Redis operation fails
        """
        try:
            redis_count = await self.count_records()
            faiss_count = await self.faiss_service.count() if self.faiss_service else 0
            
            discrepancy_found = redis_count != faiss_count
            
            result = {
                "redis_count": redis_count,
                "faiss_count": faiss_count,
                "discrepancy_found": discrepancy_found,
                "consistent": not discrepancy_found
            }
            
            if discrepancy_found:
                logger.error(f"[CONSISTENCY ERROR] Redis count ({redis_count}) != FAISS count ({faiss_count})")
            else:
                logger.info(f"[CONSISTENCY OK] Redis count ({redis_count}) == FAISS count ({faiss_count})")
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to verify data consistency: {e}")
            raise

    async def auto_verify_after_operation(self, operation_name: str) -> None:
        """
        Automatically verify data consistency after critical operations.
        
        Args:
            operation_name: Name of the operation for logging
            
        Raises:
            RedisOperationError: If consistency check fails
        """
        try:
            consistency_info = await self.verify_data_consistency()
            
            if consistency_info["discrepancy_found"]:
                logger.error(f"[AUTO VERIFY FAILED] {operation_name}: Data inconsistency detected")
                # In production, you might want to trigger alerts or recovery procedures
                # For now, we just log the error
            else:
                logger.info(f"[AUTO VERIFY PASSED] {operation_name}: Data consistency verified")
                
        except Exception as e:
            logger.error(f"Failed to auto-verify after {operation_name}: {e}")
            # Don't raise the exception to avoid breaking the main operation
            # Just log the verification failure

    async def flush_all(self):
        """
        Completely flushes the current Redis database (removes all keys).
        Use with caution in tests only!
        
        Raises:
            RedisOperationError: If Redis operation fails
        """
        return await self.crud_service.flush_all()

    # ============================================================================
    # SEARCH OPERATIONS (delegated to RedisMetadataFilterService)
    # ============================================================================

    async def search(self,
                    chunk_query: ChunkQuery,
                    limit: Optional[int] = None,
                    offset: Optional[int] = None,
                    include_vectors: bool = True,
                    include_deleted: bool = False) -> List[Dict[str, Any]]:
        """
        Unified search method using ChunkQuery for all search criteria.
        
        Args:
            chunk_query: ChunkQuery object with search criteria
            limit: Maximum number of results
            offset: Number of results to skip
            include_vectors: Whether to include vectors in results
            include_deleted: Whether to include soft-deleted records
            
        Returns:
            List of matching records with metadata and optionally vectors
            
        Raises:
            FaissSearchError: If search operation fails
            RedisOperationError: If metadata operation fails
            ValueError: If ChunkQuery is not provided
        """
        logger.debug(f"search called with include_vectors={include_vectors}")
        results = await self.filter_service.search(
            chunk_query=chunk_query,
            limit=limit,
            offset=offset,
            include_vectors=False,  # Get metadata only, restore vectors separately
            include_deleted=include_deleted
        )
        
        # Restore vectors from FAISS if requested
        if include_vectors and results:
            logger.debug(f"Restoring vectors for {len(results)} results")
            # Get faiss_idx for each result from Redis
            for result in results:
                if result and result.get('uuid'):
                    chunk_data = await self.crud_service.get_chunk(result['uuid'])
                    logger.debug(f"Raw chunk_data for {result['uuid']}: {chunk_data}")
                    if chunk_data and chunk_data.get('faiss_idx') is not None:
                        result['faiss_idx'] = chunk_data['faiss_idx']
                        logger.debug(f"Found faiss_idx {chunk_data['faiss_idx']} for {result['uuid']}")
                    else:
                        logger.debug(f"No faiss_idx found for {result['uuid']}")
            
            await self._restore_vectors_from_redis(results)
            
        return results

    # ============================================================================
    # UTILITY METHODS
    # ============================================================================

    async def get_search_stats(self) -> Dict[str, Any]:
        """
        Get search service statistics.
        
        Returns:
            Dictionary with search service statistics
        """
        return await self.filter_service.get_search_stats()

    async def ping(self) -> bool:
        """
        Check Redis connection health.
        
        Returns:
            True if connection is healthy, False otherwise
        """
        return await self.filter_service.ping()

    async def get_id_to_idx_mapping(self) -> Dict[str, int]:
        """
        Get UUID to index mapping from Redis.
        
        Returns:
            Dictionary mapping UUIDs to their corresponding indices
            
        Raises:
            Exception: If mapping retrieval fails
        """
        return await self.filter_service.get_id_to_idx_mapping()

    @property
    def vector_size(self) -> int:
        """
        Get vector dimension from FAISS service.
        
        Returns:
            Vector dimension (typically 384)
        """
        return self.filter_service.vector_size

    # ============================================================================
    # MAINTENANCE OPERATIONS (delegated to VectorStoreMaintenanceService)
    # ============================================================================

    async def deferred_cleanup(self, background: bool = True, batch_size: int = 100) -> Dict[str, Any]:
        """
        Perform deferred cleanup of soft-deleted records.
        
        Args:
            background: Whether to run in background (non-blocking)
            batch_size: Number of records to process per batch
            
        Returns:
            Dictionary with operation status and results
        """
        return await self._maintenance_service.deferred_cleanup(background, batch_size)

    async def find_duplicates(self,
                            metadata_filter: Dict[str, Any] = None,
                            batch_size: int = 1000,
                            background: bool = False) -> Dict[str, Any]:
        """
        Find duplicate records based on content similarity.
        
        Args:
            metadata_filter: Optional metadata filter to limit search scope
            batch_size: Number of records to process per batch
            background: Whether to run in background
            
        Returns:
            Dictionary with duplicate analysis results
        """
        return await self._maintenance_service.find_duplicates(metadata_filter, batch_size, background)

    async def find_duplicate_uuids(self, metadata_filter: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """
        Find UUIDs that have duplicate records using optimized Redis operations.
        
        Args:
            metadata_filter: Optional metadata filter to limit search scope
            
        Returns:
            List of dictionaries with duplicate UUIDs and their records
        """
        try:
            # Step 1: Get UUIDs based on filter
            if metadata_filter:
                # Analyze if vectors are needed in the filter
                include_vectors = self._should_include_vectors(metadata_filter)
                
                # Apply metadata filter with optimized vector loading
                filtered_records = await self.search_by_metadata(
                    metadata_filter, 
                    include_vectors=include_vectors
                )
                uuids_to_check = [record.get('uuid') for record in filtered_records if record.get('uuid')]
            else:
                # Use SCAN for efficient key retrieval
                uuids_to_check = await self._scan_uuids()
            
            if not uuids_to_check:
                return []
            
            # Step 2: Find duplicates in the same pass
            duplicates = await self._find_duplicates_in_uuids(uuids_to_check)
            
            return duplicates
            
        except Exception as e:
            logger.error(f"Failed to find duplicate UUIDs: {e}")
            return []

    def _should_include_vectors(self, metadata_filter: Dict[str, Any]) -> bool:
        """
        Analyze metadata filter to determine if vectors are needed.
        
        Args:
            metadata_filter: Metadata filter dictionary
            
        Returns:
            True if vectors are needed, False otherwise
        """
        try:
            # Check if filter contains vector-related fields
            vector_fields = {
                'embedding', 'vector', 'similarity', 'distance',
                'text', 'body', 'content'  # These might trigger vectorization
            }
            
            # Recursively check all values in the filter
            def check_for_vector_fields(obj):
                if isinstance(obj, dict):
                    for key, value in obj.items():
                        # Check if key contains vector-related terms
                        if any(vector_term in key.lower() for vector_term in vector_fields):
                            return True
                        if check_for_vector_fields(value):
                            return True
                elif isinstance(obj, list):
                    for item in obj:
                        if check_for_vector_fields(item):
                            return True
                elif isinstance(obj, str):
                    # Check if string contains vector-related terms
                    if any(vector_term in obj.lower() for vector_term in vector_fields):
                        return True
                return False
            
            return check_for_vector_fields(metadata_filter)
            
        except Exception as e:
            logger.warning(f"Failed to analyze metadata filter for vector usage: {e}")
            # Default to True if analysis fails (safe default)
            return True

    async def _find_duplicates_in_uuids(self, uuids: List[str]) -> List[Dict[str, Any]]:
        """
        Find duplicates among given UUIDs using optimized Redis operations.
        
        Args:
            uuids: List of UUIDs to check
            
        Returns:
            List of duplicate UUIDs with their records (without vectors)
        """
        try:
            # Use pipeline to check existence and get basic metadata in one pass
            # Exclude vectors to improve performance
            pipeline = await self.redis.pipeline()
            
            for uuid in uuids:
                # Check existence and get minimal metadata (no vectors)
                pipeline.exists(f"vector:{uuid}")
                pipeline.hmget(f"vector:{uuid}", "uuid", "text", "type", "body", "metadata")
            
            # Execute pipeline
            results = await pipeline.execute()
            
            # Process results in pairs (exists, metadata)
            uuid_records = {}
            for i in range(0, len(results), 2):
                exists = results[i]
                metadata = results[i + 1] if i + 1 < len(results) else None
                
                if exists and metadata:
                    uuid = uuids[i // 2]
                    
                    # Decode metadata (excluding vectors for performance)
                    record = {}
                    if metadata[0]:  # uuid
                        record["uuid"] = metadata[0].decode('utf-8') if isinstance(metadata[0], bytes) else metadata[0]
                    if metadata[1]:  # text
                        record["text"] = metadata[1].decode('utf-8') if isinstance(metadata[1], bytes) else metadata[1]
                    if metadata[2]:  # type
                        record["type"] = metadata[2].decode('utf-8') if isinstance(metadata[2], bytes) else metadata[2]
                    if metadata[3]:  # body
                        record["body"] = metadata[3].decode('utf-8') if isinstance(metadata[3], bytes) else metadata[3]
                    if metadata[4]:  # metadata
                        try:
                            metadata_json = metadata[4].decode('utf-8') if isinstance(metadata[4], bytes) else metadata[4]
                            record["metadata"] = json.loads(metadata_json) if metadata_json else {}
                        except (json.JSONDecodeError, TypeError):
                            record["metadata"] = {}
                    
                    # Group by UUID (for potential duplicates)
                    if uuid not in uuid_records:
                        uuid_records[uuid] = []
                    uuid_records[uuid].append(record)
            
            # Find UUIDs with multiple records
            duplicates = []
            for uuid, records in uuid_records.items():
                if len(records) > 1:
                    duplicates.append({
                        "uuid": uuid,
                        "records": records
                    })
            
            return duplicates
            
        except Exception as e:
            logger.error(f"Failed to find duplicates in UUIDs: {e}")
            return []

    async def _scan_uuids(self) -> List[str]:
        """
        Scan Redis for all UUIDs using SCAN operation.
        
        Returns:
            List of all UUIDs
        """
        uuids = []
        cursor = 0
        
        try:
            while True:
                # Use SCAN with pattern to get vector keys
                cursor, keys = await self.redis.scan(cursor=cursor, match="vector:*", count=1000)
                
                # Extract UUIDs from keys
                for key in keys:
                    if isinstance(key, bytes):
                        key = key.decode('utf-8')
                    uuid = key.replace("vector:", "")
                    uuids.append(uuid)
                
                # Break when scan is complete
                if cursor == 0:
                    break
                    
        except Exception as e:
            logger.error(f"Failed to scan UUIDs: {e}")
            return []
        
        return uuids

    async def reindex_all(self,
                         embedding_service,
                         lock_database: bool = True,
                         background: bool = False) -> Dict[str, Any]:
        """
        Reindex all records in the vector store.
        
        Args:
            embedding_service: Service for text vectorization
            lock_database: Whether to lock database during reindexing
            background: Whether to run in background
            
        Returns:
            Dictionary with reindexing results
        """
        return await self._maintenance_service.reindex_all(embedding_service, lock_database, background)

    async def cleanup_orphans(self, background: bool = True) -> Dict[str, Any]:
        """
        Clean up orphaned records (records in Redis but not in FAISS).
        
        Args:
            background: Whether to run in background
            
        Returns:
            Dictionary with cleanup results
        """
        return await self._maintenance_service.cleanup_orphans(background)

    async def get_maintenance_statistics(self) -> Dict[str, Any]:
        """
        Get comprehensive statistics about the vector store.
        
        Returns:
            Dictionary with statistics
        """
        return await self._maintenance_service.get_statistics()

    async def get_operation_status(self, operation_id: str) -> Dict[str, Any]:
        """
        Get status of a maintenance operation.
        
        Args:
            operation_id: Operation identifier
            
        Returns:
            Dictionary with operation status
        """
        return await self._maintenance_service.get_operation_status(operation_id)

    # ============================================================================
    # VECTOR INDEX MANAGER METHODS
    # ============================================================================

    async def is_deleted(self, uuids: List[str]) -> List[bool]:
        """
        Check if UUIDs are marked as deleted.
        
        Args:
            uuids: List of UUIDs to check
            
        Returns:
            List of boolean values indicating deletion status
        """
        try:
            if not uuids:
                return []
            
            pipeline = await self.redis.pipeline()
            for uuid in uuids:
                pipeline.hget(f"vector:{uuid}", "is_deleted")
            
            results = await pipeline.execute()
            
            # Handle different result types from Redis
            deletion_status = []
            for result in results:
                if result is None:
                    deletion_status.append(False)
                elif isinstance(result, bytes):
                    deletion_status.append(result.decode('utf-8') == 'true')
                elif isinstance(result, str):
                    deletion_status.append(result == 'true')
                elif isinstance(result, bool):
                    deletion_status.append(result)
                else:
                    deletion_status.append(False)
            
            return deletion_status
            
        except Exception as e:
            logger.error(f"Failed to check deletion status: {e}")
            return [False] * len(uuids)

    async def get_all_uuids(self, include_deleted: bool = False) -> List[str]:
        """
        Get all UUIDs from the vector store.
        
        Args:
            include_deleted: Whether to include soft-deleted records
            
        Returns:
            List of UUIDs
        """
        try:
            all_uuids = await self.crud_service.get_all_record_ids()
            
            if not include_deleted:
                # Filter out soft-deleted records
                deletion_status = await self.is_deleted(all_uuids)
                all_uuids = [uuid for uuid, is_deleted in zip(all_uuids, deletion_status) if not is_deleted]
            
            return all_uuids
            
        except Exception as e:
            logger.error(f"Failed to get UUIDs: {e}")
            return []

    async def get_deleted_uuids(self) -> List[str]:
        """
        Get UUIDs of soft-deleted records.
        
        Returns:
            List of soft-deleted UUIDs
        """
        try:
            all_uuids = await self.crud_service.get_all_record_ids()
            if not all_uuids:
                return []
            
            deletion_status = await self.is_deleted(all_uuids)
            # Ensure deletion_status is a list of booleans
            if not isinstance(deletion_status, list):
                logger.warning("is_deleted returned non-list result, treating as empty")
                return []
            
            return [uuid for uuid, is_deleted in zip(all_uuids, deletion_status) if is_deleted]
        except Exception as e:
            logger.error(f"Failed to get deleted UUIDs: {e}")
            return []

    async def get_index_info(self) -> Dict[str, Any]:
        """
        Get information about the vector store index.
        
        Returns:
            Dictionary with index information
        """
        try:
            faiss_info = await self.faiss_service.get_index_info()
            redis_count = await self.crud_service.count_records()
            
            return {
                'faiss': faiss_info,
                'redis_records': redis_count,
                'vector_size': self.vector_size,
                'total_vectors': faiss_info.get('total_vectors', 0)
            }
        except Exception as e:
            logger.error(f"Failed to get index info: {e}")
            return {}

    async def info(self) -> Dict[str, Any]:
        """
        Get comprehensive information about the vector store.
        
        Returns:
            Dictionary with detailed statistics including:
            - UUID counts (total, active, deleted)
            - Source ID statistics
            - FAISS metrics
            - Redis metrics
            - Metadata manager statistics
        """
        try:
            # Basic counts
            all_uuids = await self.get_all_uuids(include_deleted=True)
            deleted_uuids = await self.get_deleted_uuids()
            active_uuids = await self.get_all_uuids(include_deleted=False)
            
            # Source ID statistics
            source_id_stats = await self._get_source_id_statistics(all_uuids)
            
            # FAISS information
            faiss_info = await self.faiss_service.get_index_info()
            
            # Redis statistics
            redis_stats = await self._get_redis_statistics()
            
            # Metadata manager statistics
            metadata_stats = await self._get_metadata_statistics()
            
            # Search statistics
            search_stats = await self.get_search_stats()
            
            # New adapter metrics
            cache_metrics = await self._get_cache_metrics()
            performance_metrics = await self._get_performance_metrics()
            complexity_metrics = await self._get_complexity_metrics()
            error_metrics = await self._get_error_metrics()
            resource_metrics = await self._get_resource_metrics()
            business_metrics = await self._get_business_metrics()
            
            return {
                'uuid_statistics': {
                    'total_uuids': len(all_uuids),
                    'active_uuids': len(active_uuids),
                    'deleted_uuids': len(deleted_uuids),
                    'deletion_rate': len(deleted_uuids) / len(all_uuids) if all_uuids else 0
                },
                'source_id_statistics': source_id_stats,
                'faiss_statistics': {
                    'total_vectors': faiss_info.get('total_vectors', 0),
                    'vector_size': faiss_info.get('vector_size', 0),
                    'index_type': faiss_info.get('index_type', 'Unknown'),
                    'operations_since_save': faiss_info.get('operations_since_save', 0),
                    'last_save_time': faiss_info.get('last_save_time'),
                    'auto_save_enabled': faiss_info.get('auto_save_enabled', False)
                },
                'redis_statistics': redis_stats,
                'metadata_statistics': metadata_stats,
                'search_statistics': search_stats,
                'system_info': {
                    'vector_size': self.vector_size,
                    'has_embedding_service': self.embedding_service is not None,
                    'has_crud_service': self.crud_service is not None,
                    'has_filter_service': self.filter_service is not None,
                    'has_maintenance_service': hasattr(self, 'maintenance_service') and self.maintenance_service is not None
                },
                'cache_metrics': cache_metrics,
                'performance_metrics': performance_metrics,
                'complexity_metrics': complexity_metrics,
                'error_metrics': error_metrics,
                'resource_metrics': resource_metrics,
                'business_metrics': business_metrics
            }
        except Exception as e:
            logger.error(f"Failed to get comprehensive info: {e}")
            return {'error': str(e)}

    async def _get_source_id_statistics(self, uuids: List[str]) -> Dict[str, Any]:
        """
        Get statistics about source_id distribution.
        
        Args:
            uuids: List of UUIDs to analyze
            
        Returns:
            Dictionary with source_id statistics
        """
        try:
            if not uuids:
                return {
                    'total_source_ids': 0,
                    'unique_source_ids': 0,
                    'source_id_distribution': {},
                    'most_common_source_ids': []
                }
            
            # Get chunks in batches to avoid memory issues
            batch_size = 100
            source_ids = []
            
            for i in range(0, len(uuids), batch_size):
                batch_uuids = uuids[i:i + batch_size]
                chunks = await self.get_chunks(batch_uuids)
                
                for chunk in chunks:
                    if chunk and 'source_id' in chunk:
                        source_ids.append(chunk['source_id'])
            
            # Calculate statistics
            from collections import Counter
            source_id_counter = Counter(source_ids)
            
            return {
                'total_source_ids': len(source_ids),
                'unique_source_ids': len(source_id_counter),
                'source_id_distribution': dict(source_id_counter),
                'most_common_source_ids': source_id_counter.most_common(10),
                'source_id_counts': dict(source_id_counter)
            }
        except Exception as e:
            logger.error(f"Failed to get source_id statistics: {e}")
            return {'error': str(e)}

    async def _get_redis_statistics(self) -> Dict[str, Any]:
        """
        Get Redis statistics.
        
        Returns:
            Dictionary with Redis statistics
        """
        try:
            # Get all vector keys
            vector_keys = await self.redis.keys("vector:*")
            
            # Get memory usage for all keys
            total_memory = 0
            for key in vector_keys:
                try:
                    memory = await self.redis.memory_usage(key)
                    if memory:
                        total_memory += memory
                except:
                    pass
            
            # Get deleted keys count
            deleted_keys = await self.redis.keys("deleted:*")
            
            return {
                'total_keys': len(vector_keys),
                'deleted_keys': len(deleted_keys),
                'estimated_memory_usage_bytes': total_memory,
                'redis_connection_healthy': await self.ping(),
                'total_records': len(vector_keys)
            }
        except Exception as e:
            logger.error(f"Failed to get Redis statistics: {e}")
            return {'error': str(e)}

    async def _get_metadata_statistics(self) -> Dict[str, Any]:
        """
        Get metadata manager statistics.
        
        Returns:
            Dictionary with metadata statistics
        """
        try:
            # Get index fields if available
            index_fields = []
            try:
                fields_result = await self.crud_service.index_manager.get_index_fields("main_idx")
                if fields_result.get("status"):
                    index_fields = fields_result.get("result", [])
            except:
                pass
            
            # Get index statistics if available
            index_stats = {}
            try:
                stats_result = await self.crud_service.index_manager._search.get_index_stats("main_idx")
                if stats_result:
                    index_stats = stats_result
            except:
                pass
            
            return {
                'indexed_fields': index_fields,
                'index_statistics': index_stats,
                'has_index_manager': hasattr(self.crud_service, 'index_manager'),
                'metadata_field_counts': {}
            }
        except Exception as e:
            logger.error(f"Failed to get metadata statistics: {e}")
            return {'error': str(e)}

    async def _get_cache_metrics(self) -> Dict[str, Any]:
        """
        Get cache performance metrics from chunk_metadata_adapter.
        
        Returns:
            Dictionary with cache metrics
        """
        try:
            cache_metrics = {}
            
            # QueryCache metrics (if available)
            try:
                from chunk_metadata_adapter.ast import QueryCache
                # Note: This would require access to actual cache instances
                # For now, return placeholder data
                cache_metrics['query_cache'] = {
                    'hits': 0,
                    'misses': 0,
                    'evictions': 0,
                    'size': 0,
                    'max_size': 100,
                    'hit_rate': 0.0
                }
            except ImportError:
                cache_metrics['query_cache'] = {'error': 'QueryCache not available'}
            
            # FilterExecutor cache metrics (if available)
            try:
                from chunk_metadata_adapter.filter_executor import FilterExecutor
                # Note: This would require access to actual executor instances
                cache_metrics['filter_executor_cache'] = {
                    'field_cache_size': 0,
                    'comparison_cache_size': 0,
                    'total_cache_entries': 0
                }
            except ImportError:
                cache_metrics['filter_executor_cache'] = {'error': 'FilterExecutor not available'}
            
            # ChunkQuery cache metrics (if available)
            try:
                from chunk_metadata_adapter.chunk_query import ChunkQuery
                # Note: This would require access to actual query instances
                cache_metrics['chunk_query_cache'] = {
                    'ast_cached': False,
                    'validation_cached': False,
                    'parser_initialized': True,
                    'executor_initialized': True,
                    'validator_initialized': True,
                    'optimizer_initialized': True
                }
            except ImportError:
                cache_metrics['chunk_query_cache'] = {'error': 'ChunkQuery not available'}
            
            # Add cache_hit_rate field
            cache_metrics['cache_hit_rate'] = 0.0
            
            return cache_metrics
        except Exception as e:
            logger.error(f"Failed to get cache metrics: {e}")
            return {'error': str(e)}

    async def _get_performance_metrics(self) -> Dict[str, Any]:
        """
        Get performance metrics from chunk_metadata_adapter.
        
        Returns:
            Dictionary with performance metrics
        """
        try:
            performance_metrics = {}
            
            # Query parsing performance (placeholder)
            performance_metrics['query_parsing'] = {
                'average_parse_time_ms': 0.5,
                'total_queries_parsed': 0,
                'parse_time_distribution': {'fast': 0, 'medium': 0, 'slow': 0}
            }
            
            # Filter execution performance (placeholder)
            performance_metrics['filter_execution'] = {
                'average_execution_time_ms': 0.2,
                'total_filters_executed': 0,
                'execution_time_distribution': {'fast': 0, 'medium': 0, 'slow': 0}
            }
            
            # Add avg_response_time field
            performance_metrics['avg_response_time'] = 0.7
            
            return performance_metrics
        except Exception as e:
            logger.error(f"Failed to get performance metrics: {e}")
            return {'error': str(e)}

    async def _get_complexity_metrics(self) -> Dict[str, Any]:
        """
        Get complexity analysis metrics from chunk_metadata_adapter.
        
        Returns:
            Dictionary with complexity metrics
        """
        try:
            complexity_metrics = {}
            
            # AST analysis metrics (placeholder)
            complexity_metrics['ast_analysis'] = {
                'average_max_depth': 0.0,
                'average_total_conditions': 0.0,
                'complexity_distribution': {'simple': 0, 'medium': 0, 'complex': 0}
            }
            
            # Operator usage statistics (placeholder)
            complexity_metrics['operator_usage'] = {
                'AND': 0,
                'OR': 0,
                '=': 0,
                '>=': 0,
                'intersects': 0
            }
            
            # Field usage statistics (placeholder)
            complexity_metrics['field_usage'] = {
                'type': 0,
                'quality_score': 0,
                'tags': 0,
                'year': 0,
                'status': 0
            }
            
            # Add avg_query_complexity field
            complexity_metrics['avg_query_complexity'] = 0.0
            
            return complexity_metrics
        except Exception as e:
            logger.error(f"Failed to get complexity metrics: {e}")
            return {'error': str(e)}

    async def _get_error_metrics(self) -> Dict[str, Any]:
        """
        Get error statistics from chunk_metadata_adapter.
        
        Returns:
            Dictionary with error metrics
        """
        try:
            error_metrics = {}
            
            # Validation errors (placeholder)
            error_metrics['validation_errors'] = {
                'total_queries': 0,
                'valid_queries': 0,
                'invalid_queries': 0,
                'error_rate': 0.0,
                'error_types': {
                    'syntax_error': 0,
                    'invalid_field': 0,
                    'unsupported_operator': 0,
                    'security_violation': 0
                }
            }
            
            # Execution errors (placeholder)
            error_metrics['execution_errors'] = {
                'total_executions': 0,
                'successful_executions': 0,
                'failed_executions': 0,
                'error_rate': 0.0
            }
            
            # Add total_errors field
            error_metrics['total_errors'] = 0
            
            return error_metrics
        except Exception as e:
            logger.error(f"Failed to get error metrics: {e}")
            return {'error': str(e)}

    async def _get_resource_metrics(self) -> Dict[str, Any]:
        """
        Get resource usage metrics from chunk_metadata_adapter.
        
        Returns:
            Dictionary with resource metrics
        """
        try:
            resource_metrics = {}
            
            # Memory usage estimation (placeholder)
            resource_metrics['memory_usage'] = {
                'query_cache_bytes': 0,
                'filter_executor_cache_bytes': 0,
                'total_cache_memory_bytes': 0,
                'estimated_total_memory_bytes': 0
            }
            
            # Cache efficiency (placeholder)
            resource_metrics['cache_efficiency'] = {
                'query_cache_hit_rate': 0.0,
                'filter_cache_hit_rate': 0.0,
                'overall_cache_hit_rate': 0.0
            }
            
            # Add cpu_usage field
            resource_metrics['cpu_usage'] = 0.0
            
            return resource_metrics
        except Exception as e:
            logger.error(f"Failed to get resource metrics: {e}")
            return {'error': str(e)}

    async def _get_business_metrics(self) -> Dict[str, Any]:
        """
        Get business usage patterns from chunk_metadata_adapter.
        
        Returns:
            Dictionary with business metrics
        """
        try:
            business_metrics = {}
            
            # Query patterns (placeholder)
            business_metrics['query_patterns'] = {
                'most_common_fields': [],
                'most_common_operators': [],
                'query_complexity_trend': {'simple': 0, 'medium': 0, 'complex': 0}
            }
            
            # Usage statistics (placeholder)
            business_metrics['usage_statistics'] = {
                'total_queries_today': 0,
                'total_queries_this_week': 0,
                'peak_queries_per_hour': 0,
                'average_queries_per_hour': 0.0
            }
            
            return business_metrics
        except Exception as e:
            logger.error(f"Failed to get business metrics: {e}")
            return {'error': str(e)}

    # ============================================================================
    # BACKWARD COMPATIBILITY METHODS
    # ============================================================================

    async def save_metadata(self, uuid: str, metadata: Dict[str, Any], idx: int = None) -> bool:
        """
        Save metadata for backward compatibility.
        
        Args:
            uuid: Chunk UUID
            metadata: Metadata dictionary
            idx: FAISS index (optional)
            
        Returns:
            True if saved successfully
            
        Raises:
            RedisOperationError: If Redis operation fails
        """
        try:
            # Create SemanticChunk from metadata with required fields
            chunk_data = {
                "uuid": uuid,
                "type": "DocBlock",  # Default type
                "body": metadata.get("body", ""),
                "text": metadata.get("text", ""),
                "metadata": metadata,
                "source_id": metadata.get("source_id", ""),
                "chunk_id": metadata.get("chunk_id", ""),
                "embedding": metadata.get("embedding", [])
            }
            chunk = SemanticChunk(**chunk_data)
            
            # Save using CRUD service
            saved_uuid = await self.crud_service.upsert_chunk(chunk)
            return saved_uuid == uuid
        except Exception as e:
            logger.error(f"Failed to save metadata: {e}")
            return False

    async def get_metadata(self, uuid: str) -> Optional[Dict[str, Any]]:
        """
        Get metadata for backward compatibility.
        
        Args:
            uuid: Chunk UUID
            
        Returns:
            Metadata dictionary or None if not found
            
        Raises:
            RedisOperationError: If Redis operation fails
        """
        chunk_data = await self.crud_service.get_chunk(uuid)
        if chunk_data and 'uuid' in chunk_data:
            # Remove uuid from metadata to match expected format
            metadata = {k: v for k, v in chunk_data.items() if k != 'uuid'}
            return metadata
        return chunk_data

    async def delete_record(self, uuid: str) -> bool:
        """
        Delete record for backward compatibility.
        
        Args:
            uuid: Chunk UUID
            
        Returns:
            True if deleted successfully
            
        Raises:
            RedisOperationError: If Redis operation fails
        """
        return await self.crud_service.delete_chunk(uuid)

    def is_valid_uuid4(self, uuid_str: str) -> bool:
        """
        Check if string is valid UUID4 for backward compatibility.
        
        Args:
            uuid_str: String to validate
            
        Returns:
            True if valid UUID4, False otherwise
        """
        if uuid_str is None:
            return False
        from vector_store.utils.validation import is_valid_uuid4
        result = is_valid_uuid4(uuid_str)
        return result is not None 

    def _format_vector_log(self, vector, max_show=3):
        """
        Format vector for logging - show dimension and first few values.
        
        Args:
            vector: Vector to format
            max_show: Maximum number of values to show
            
        Returns:
            Formatted string for logging
        """
        if not vector:
            return "empty"
        if isinstance(vector, (list, tuple)):
            if len(vector) == 0:
                return "empty"
            preview = vector[:max_show]
            return f"dim={len(vector)}, values={preview}{'...' if len(vector) > max_show else ''}"
        return f"type={type(vector)}"
    
    async def _restore_vectors_from_redis(self, chunks_data: List[Dict[str, Any]]) -> None:
        """
        Restore vectors from Redis for chunks.
        
        Args:
            chunks_data: List of chunk dictionaries from Redis
        """
        logger.debug(f"_restore_vectors_from_redis called with {len(chunks_data)} chunks")
        
        if not chunks_data:
            logger.debug("No chunks data")
            return
            
        try:
            # Get vectors from Redis for all chunks
            uuids = [chunk['uuid'] for chunk in chunks_data if chunk and 'uuid' in chunk]
            logger.debug(f"Getting vectors from Redis for {len(uuids)} UUIDs")
            
            if not uuids:
                logger.debug("No UUIDs found")
                return
                
            # Get vectors from Redis
            vectors = await self.crud_service.get_vectors_from_redis(uuids)
            logger.debug(f"Retrieved {len(vectors)} vectors from Redis")
            if vectors and vectors[0]:
                logger.debug(f"First vector: {self._format_vector_log(vectors[0])}")
            else:
                logger.debug("First vector: None")
            
            # Map vectors to chunks
            uuid_to_vector = dict(zip(uuids, vectors))
            
            for chunk in chunks_data:
                if chunk and 'uuid' in chunk:
                    uuid = chunk['uuid']
                    vector = uuid_to_vector.get(uuid)
                    
                    if vector is not None:
                        chunk['embedding'] = vector
                        logger.debug(f"Set embedding for {uuid}: {self._format_vector_log(vector)}")
                    else:
                        # Keep existing embedding if any, otherwise set empty
                        if 'embedding' not in chunk or not chunk['embedding']:
                            chunk['embedding'] = []
                            logger.debug(f"No vector found in Redis for {uuid}, setting empty embedding")
                        else:
                            logger.debug(f"Keeping existing embedding for {uuid}: {self._format_vector_log(chunk['embedding'])}")
                    
        except Exception as e:
            logger.error(f"Failed to restore vectors from Redis: {e}")
            # Set empty embeddings for all chunks if Redis retrieval fails
            for chunk in chunks_data:
                if chunk and ('embedding' not in chunk or not chunk['embedding']):
                    chunk['embedding'] = []
                    logger.debug(f"Setting empty embedding for {chunk.get('uuid', 'unknown')} due to Redis error")
                else:
                    logger.debug(f"Keeping embedding for {chunk.get('uuid', 'unknown')}: {self._format_vector_log(chunk.get('embedding', []))}") 

    async def full_reindex(self, embedding_service) -> None:
        """
          FAISS     Redis,   text.
        
        Args:
            embedding_service:    
        """
        try:
            # 1.   UUID  Redis
            uuids = await self.crud_service.get_all_record_ids()
            if not uuids:
                logger.info("No chunks found for reindexing")
                return
                
            # 2.    Redis 
            chunks = await self.crud_service.get_chunks(uuids)
            texts = [(chunk['uuid'], chunk['text']) for chunk in chunks if chunk and chunk.get('text')]
            
            if not texts:
                logger.info("No chunks with text found for reindexing")
                return
                
            # 3.      FAISS
            if embedding_service and self.faiss_service:
                logger.info(f"Reindexing {len(texts)} chunks")
                
                #  
                vectors = []
                uuids_to_index = []
                
                for uuid, text in texts:
                    try:
                        #  embedding  
                        embedding = await embedding_service.get_embedding(text)
                        if embedding is not None:
                            vectors.append(embedding)
                            uuids_to_index.append(uuid)
                    except Exception as e:
                        logger.warning(f"Failed to get embedding for UUID {uuid}: {e}")
                        continue
                
                if vectors:
                    #    FAISS
                    try:
                        indices = await self.faiss_service.add_vectors(vectors)
                        
                        #   FAISS  Redis
                        idx_pipeline = await self.crud_service.redis.pipeline()
                        for idx, uuid in zip(indices, uuids_to_index):
                            idx_pipeline.set(f"faiss_idx:{idx}", uuid)
                            idx_pipeline.hset(self.crud_service.chunk_key(uuid), "faiss_idx", idx)
                        await idx_pipeline.execute()
                        
                        logger.info(f"Successfully reindexed {len(indices)} chunks in FAISS")
                    except Exception as e:
                        logger.error(f"Failed to add vectors to FAISS during reindex: {e}")
                        raise
                else:
                    logger.warning("No valid vectors generated during reindex")
            else:
                logger.warning("No embedding service or FAISS service available for reindex")
                
        except Exception as e:
            logger.error(f"Failed to perform full reindex: {e}")
            raise 

    # ============================================================================
    # BM25 OPERATIONS
    # ============================================================================

    async def initialize_bm25(self) -> None:
        """
        Initialize BM25 search service.
        
        Raises:
            ServiceInitializationError: If BM25 initialization fails
        """
        if not self.bm25_service:
            raise ServiceInitializationError("VectorStoreService", "BM25 service not configured")
        
        try:
            await self.bm25_service.initialize()
            logger.info("BM25 search service initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize BM25 service: {e}")
            raise ServiceInitializationError("VectorStoreService", f"BM25 initialization failed: {e}")

    async def index_chunk_for_bm25(self, chunk: SemanticChunk) -> None:
        """
        Index chunk for BM25 search.
        
        Args:
            chunk: SemanticChunk to index
            
        Raises:
            ServiceInitializationError: If BM25 service not available
        """
        if not self.bm25_service:
            raise ServiceInitializationError("VectorStoreService", "BM25 service not configured")
        
        try:
            # Index both text and body fields
            if chunk.text:
                await self.bm25_service.index_document(chunk.uuid, chunk.text)
            if chunk.body:
                await self.bm25_service.index_document(chunk.uuid, chunk.body)
            
            logger.debug(f"Indexed chunk {chunk.uuid} for BM25 search")
        except Exception as e:
            logger.error(f"Failed to index chunk {chunk.uuid} for BM25: {e}")

    async def search_bm25(self, query: str, limit: int = 10, k1: float = 1.2, b: float = 0.75) -> List[Dict[str, Any]]:
        """
        Search using BM25 algorithm.
        
        Args:
            query: Search query text
            limit: Maximum number of results
            k1: BM25 k1 parameter
            b: BM25 b parameter
            
        Returns:
            List of search results with scores
            
        Raises:
            ServiceInitializationError: If BM25 service not available
        """
        if not self.bm25_service:
            raise ServiceInitializationError("VectorStoreService", "BM25 service not configured")
        
        try:
            results = await self.bm25_service.search(query, limit=limit, k1=k1, b=b)
            
            # Convert to standard format
            formatted_results = []
            for result in results:
                formatted_results.append({
                    'uuid': result.doc_id,
                    'score': result.score,
                    'rank': result.rank
                })
            
            return formatted_results
        except Exception as e:
            logger.error(f"BM25 search failed: {e}")
            return []

    async def hybrid_search(self, chunk_query: ChunkQuery) -> List[Dict[str, Any]]:
        """
        Perform hybrid search combining BM25 and semantic search.
        
        Args:
            chunk_query: ChunkQuery with search parameters
            
        Returns:
            List of search results with combined scores
        """
        if not self.bm25_service:
            # Fallback to semantic search only
            return await self.search(chunk_query)
        
        try:
            # Get semantic search results
            semantic_results = await self.search(chunk_query)
            
            # Get BM25 search results if query provided
            bm25_results = []
            if chunk_query.search_query:
                bm25_results = await self.search_bm25(
                    chunk_query.search_query,
                    limit=chunk_query.max_results or 10,
                    k1=chunk_query.bm25_k1 or 1.2,
                    b=chunk_query.bm25_b or 0.75
                )
            
            # Combine results if both available
            if semantic_results and bm25_results:
                return await self._combine_hybrid_results(
                    semantic_results, bm25_results, chunk_query
                )
            elif bm25_results:
                return bm25_results
            else:
                return semantic_results
                
        except Exception as e:
            logger.error(f"Hybrid search failed: {e}")
            # Fallback to semantic search
            return await self.search(chunk_query)

    async def _combine_hybrid_results(
        self,
        semantic_results: List[Dict[str, Any]],
        bm25_results: List[Dict[str, Any]],
        chunk_query: ChunkQuery
    ) -> List[Dict[str, Any]]:
        """
        Combine semantic and BM25 search results.
        
        Args:
            semantic_results: Results from semantic search
            bm25_results: Results from BM25 search
            chunk_query: Original query with fusion parameters
            
        Returns:
            Combined results with normalized scores
        """
        try:
            from chunk_metadata_adapter.hybrid_search import HybridSearchHelper
            
            # Create result mappings
            semantic_map = {result['uuid']: result for result in semantic_results}
            bm25_map = {result['uuid']: result for result in bm25_results}
            
            # Get all unique UUIDs
            all_uuids = set(semantic_map.keys()) | set(bm25_map.keys())
            
            # Combine results
            combined_results = []
            for uuid in all_uuids:
                semantic_score = semantic_map.get(uuid, {}).get('similarity', 0.0)
                bm25_score = bm25_map.get(uuid, {}).get('score', 0.0)
                
                # Normalize scores using min-max normalization
                # For semantic scores (typically 0-1), we can use as-is or normalize if needed
                # For BM25 scores (can be any positive value), we need to normalize
                normalized_semantic = max(0.0, min(1.0, semantic_score))  # Clamp to 0-1
                normalized_bm25 = max(0.0, min(1.0, bm25_score / 10.0))  # Normalize BM25 score (assuming max ~10)
                
                # Apply weights
                semantic_weight = chunk_query.semantic_weight or 0.5
                bm25_weight = chunk_query.bm25_weight or 0.5
                
                combined_score = (
                    normalized_semantic * semantic_weight +
                    normalized_bm25 * bm25_weight
                )
                
                # Get chunk data
                chunk_data = semantic_map.get(uuid, bm25_map.get(uuid, {}))
                
                combined_results.append({
                    'uuid': uuid,
                    'similarity': combined_score,
                    'semantic_score': semantic_score,
                    'bm25_score': bm25_score,
                    **{k: v for k, v in chunk_data.items() if k not in ['uuid', 'similarity', 'score']}
                })
            
            # Sort by combined score
            combined_results.sort(key=lambda x: x['similarity'], reverse=True)
            
            # Apply limit
            limit = chunk_query.max_results or 10
            return combined_results[:limit]
            
        except Exception as e:
            logger.error(f"Failed to combine hybrid results: {e}")
            return semantic_results 