"""
Author: Vasiliy Zdanovskiy
email: vasilyvz@gmail.com

Radial profile computation for stepwise power law analysis.

This module implements radial profile computation methods for analyzing
the stepwise structure of phase fields, supporting both CPU and CUDA acceleration.

Theoretical Background:
    Radial profiles A(r) are computed by averaging field values over
    spherical shells centered at defects, enabling analysis of decay
    behavior and layer structure in 7D space-time.

Example:
    >>> profiler = RadialProfileComputer(use_cuda=True)
    >>> profile = profiler.compute(field, center)
"""

import numpy as np
from typing import Dict, List
import logging
import sys

# CUDA support
try:
    import cupy as cp

    CUDA_AVAILABLE = True
except ImportError:
    CUDA_AVAILABLE = False
    cp = None


class RadialProfileComputer:
    """
    Radial profile computation with CUDA acceleration.

    Physical Meaning:
        Computes radial profiles by averaging field values over spherical
        shells, providing the basis for analyzing decay behavior and
        layer structure in the phase field.

    Mathematical Foundation:
        For a field a(x), the radial profile A(r) is computed as:
        A(r) = (1/V_r) âˆ«_{|x-c|=r} |a(x)| dS
        where V_r is the volume of the spherical shell at radius r.
    """

    def __init__(self, use_cuda: bool = True, gpu_memory_ratio: float = 0.8):
        """
        Initialize radial profile computer.

        Physical Meaning:
            Sets up computer with CUDA acceleration for efficient
            computation of radial profiles in 7D phase fields.

        Args:
            use_cuda (bool): Whether to use CUDA acceleration.
            gpu_memory_ratio (float): GPU memory utilization ratio (0-1).
        """
        self.use_cuda = use_cuda and CUDA_AVAILABLE
        self.gpu_memory_ratio = gpu_memory_ratio
        self.logger = logging.getLogger(__name__)

        if self.use_cuda:
            self.xp = cp
            try:
                from ....utils.cuda_utils import get_global_backend

                self.backend = get_global_backend()
            except ImportError:
                self.backend = None
        else:
            self.xp = np
            self.backend = None

    def compute(self, field: np.ndarray, center: List[float]) -> Dict[str, np.ndarray]:
        """
        Compute radial profile of the field with automatic swap/blocking.

        Physical Meaning:
            Computes the radial profile A(r) by averaging the field
            over spherical shells centered at the defect. Automatically
            uses FieldArray for transparent swap and block processing
            when field size exceeds GPU memory.

        Args:
            field (np.ndarray): 3D or 7D field array (will be wrapped in FieldArray
                for automatic swap management if needed).
            center (List[float]): Center coordinates [x, y, z].

        Returns:
            Dict[str, np.ndarray]: Radial profile with 'r' and 'A' arrays.
        """
        import sys
        field_size_mb = field.nbytes / (1024**2) if hasattr(field, 'nbytes') else 0
        self.logger.info(
            f"[RADIAL PROFILE] compute: START - field shape={field.shape}, "
            f"size={field_size_mb:.2f}MB, center={center}, use_cuda={self.use_cuda}"
        )
        sys.stdout.flush()
        sys.stderr.flush()
        
        # Automatically wrap field in FieldArray for transparent swap management
        from bhlff.core.arrays.field_array import FieldArray

        self.logger.info(f"[RADIAL PROFILE] STEP 1: Wrapping field in FieldArray if needed...")
        sys.stdout.flush()
        sys.stderr.flush()
        if not isinstance(field, FieldArray):
            # Wrap in FieldArray - it will automatically use swap if field is large
            field_wrapped = FieldArray(array=field)
            self.logger.info(f"[RADIAL PROFILE] STEP 1 COMPLETE: Field wrapped in FieldArray")
        else:
            field_wrapped = field
            self.logger.info(f"[RADIAL PROFILE] STEP 1 COMPLETE: Field already FieldArray")
        sys.stdout.flush()
        sys.stderr.flush()

        # Extract array (may be memory-mapped if swapped)
        self.logger.info(f"[RADIAL PROFILE] STEP 2: Extracting array...")
        sys.stdout.flush()
        sys.stderr.flush()
        field_array = field_wrapped.array
        is_swapped = isinstance(field_array, np.memmap)
        self.logger.info(
            f"[RADIAL PROFILE] STEP 2 COMPLETE: Array extracted, shape={field_array.shape if field_array is not None else None}, "
            f"is_swapped={is_swapped}, type={type(field_array).__name__}"
        )
        sys.stdout.flush()
        sys.stderr.flush()

        # CRITICAL: For 7D fields, ALWAYS use window-based processing to maximize GPU utilization
        # This ensures large windows (80% GPU memory) are processed entirely on GPU
        if self.use_cuda and len(field_array.shape) == 7:
            self.logger.info(
                f"[RADIAL PROFILE] STEP 3: 7D field detected, using window-based GPU processing "
                f"(is_swapped={is_swapped})"
            )
            sys.stdout.flush()
            return self._compute_cuda_with_swap(field_array, center, field_array.shape[:3])

        if self.use_cuda:
            self.logger.info(
                f"[RADIAL PROFILE] GPU MODE: Using _compute_cuda for non-7D field. "
                f"Field shape={field_array.shape}, size={field_array.nbytes/1e9:.3f}GB"
            )
            sys.stdout.flush()
            return self._compute_cuda(field_array, center)
        else:
            self.logger.warning(
                f"[RADIAL PROFILE] CPU MODE: use_cuda=False, using _compute_cpu. "
                f"Field shape={field_array.shape}, size={field_array.nbytes/1e9:.3f}GB"
            )
            sys.stdout.flush()
            return self._compute_cpu(field_array, center)

    def compute_substrate(
        self, substrate: np.ndarray, center: List[float]
    ) -> Dict[str, np.ndarray]:
        """
        Compute radial profile of substrate transparency.

        Physical Meaning:
            Computes the radial profile T(r) by averaging the substrate
            transparency over spherical shells centered at the defect
            using vectorized operations for efficiency.

        Args:
            substrate (np.ndarray): 7D substrate field.
            center (List[float]): Center coordinates [x, y, z].

        Returns:
            Dict[str, np.ndarray]: Radial profile with 'r' and 'A' arrays.
        """
        # Always use CUDA if enabled, converting numpy arrays to cupy
        use_cuda_here = self.use_cuda
        xp = self.xp if use_cuda_here else np
        
        # Convert numpy array to cupy if CUDA is enabled
        if use_cuda_here and isinstance(substrate, np.ndarray):
            substrate = xp.asarray(substrate)

        if len(substrate.shape) == 7:
            shape = substrate.shape[:3]
        else:
            shape = substrate.shape[:3]

        x = xp.arange(shape[0], dtype=xp.float32)
        y = xp.arange(shape[1], dtype=xp.float32)
        z = xp.arange(shape[2], dtype=xp.float32)
        X, Y, Z = xp.meshgrid(x, y, z, indexing="ij")

        center_array = xp.array(center, dtype=xp.float32)
        distances = xp.sqrt(
            (X - center_array[0]) ** 2
            + (Y - center_array[1]) ** 2
            + (Z - center_array[2]) ** 2
        )

        if len(substrate.shape) == 7:
            center_phi = substrate.shape[3] // 2
            center_t = substrate.shape[6] // 2
            transparency = xp.abs(
                substrate[:, :, :, center_phi, center_phi, center_phi, center_t]
            )
        else:
            transparency = xp.abs(substrate)

        r_max = float(xp.max(distances))
        num_bins = max(20, min(100, int(r_max * 10)))
        r_bins = xp.linspace(0.0, r_max, num_bins + 1)
        r_centers = (r_bins[:-1] + r_bins[1:]) / 2.0

        distances_flat = distances.ravel()
        transparency_flat = transparency.ravel()
        bin_indices = xp.searchsorted(r_bins[1:], distances_flat, side="right")
        bin_indices = xp.clip(bin_indices, 0, num_bins - 1)

        T_radial = xp.zeros(num_bins, dtype=xp.float32)
        if hasattr(xp, "bincount"):
            bin_sums = xp.bincount(
                bin_indices, weights=transparency_flat, minlength=num_bins
            )
            bin_counts = xp.bincount(bin_indices, minlength=num_bins)
            valid_mask = bin_counts > 0
            T_radial[valid_mask] = bin_sums[valid_mask] / bin_counts[valid_mask]
        else:
            for i in range(num_bins):
                mask = bin_indices == i
                if xp.any(mask):
                    T_radial[i] = xp.mean(transparency_flat[mask])

        # Always convert back to numpy for return
        if use_cuda_here:
            T_radial = cp.asnumpy(T_radial)
            r_centers = cp.asnumpy(r_centers)

        return {"r": r_centers, "A": T_radial}


    def _compute_cuda_single_block(
        self, block: np.ndarray, center: List[float], stream=None
    ) -> Dict[str, np.ndarray]:
        """
        Compute radial profile for a single block on GPU (direct, no swap check).

        Physical Meaning:
            Computes radial profile for a single spatial block on GPU,
            used in block processing pipeline. Block should already be on GPU.

        Args:
            block (np.ndarray or cp.ndarray): Block array (should be on GPU).
            center (List[float]): Center coordinates relative to block.

        Returns:
            Dict[str, np.ndarray]: Radial profile.
        """
        if len(block.shape) == 7:
            block_shape = block.shape[:3]
        else:
            block_shape = block.shape[:3]

        # STEP-BY-STEP LOGGING: Track every computation step
        import sys
        
        # Ensure block is on GPU - use cupy directly for all operations
        if self.use_cuda and CUDA_AVAILABLE:
            self.logger.info("[RADIAL COMPUTE] STEP 1: Ensuring block is on GPU")
            sys.stdout.flush()
            # Block should already be on GPU, but ensure it's cupy array
            if not isinstance(block, cp.ndarray):
                self.logger.warning(
                    f"[RADIAL COMPUTE] Block not on GPU! Type: {type(block)}, "
                    f"transferring to GPU..."
                )
                if stream is not None:
                    with stream:
                        block_gpu = cp.asarray(block)
                else:
                    block_gpu = cp.asarray(block)
            else:
                block_gpu = block
            xp = cp
            
            # CRITICAL: Verify block is actually on GPU
            if not isinstance(block_gpu, cp.ndarray):
                raise RuntimeError(
                    f"Block not on GPU after transfer! Type: {type(block_gpu)}, "
                    f"expected cp.ndarray"
                )
            
            # DEBUG: Detailed GPU memory check
            try:
                mem_info_detailed = cp.cuda.runtime.memGetInfo()
                free_mem_detailed = mem_info_detailed[0] / 1e9
                total_mem_detailed = mem_info_detailed[1] / 1e9
                used_mem_detailed = total_mem_detailed - free_mem_detailed
                block_mem = block_gpu.nbytes / 1e9
                self.logger.info(
                f"[RADIAL COMPUTE] Block on GPU: shape={block_gpu.shape}, "
                    f"dtype={block_gpu.dtype}, type={type(block_gpu).__name__}, "
                    f"block size: {block_mem:.3f}GB, GPU used: {used_mem_detailed:.2f}GB / "
                    f"{total_mem_detailed:.2f}GB ({used_mem_detailed/total_mem_detailed*100:.1f}%), "
                    f"free: {free_mem_detailed:.2f}GB"
                )
                sys.stdout.flush()
                
                # CRITICAL: Verify block is actually using GPU memory
                if block_mem > 0.01 and used_mem_detailed < 0.1:  # Block > 10MB but GPU < 100MB used
                    self.logger.warning(
                        f"[RADIAL COMPUTE WARNING] Block size {block_mem:.3f}GB but GPU only "
                        f"using {used_mem_detailed:.2f}GB! Block may not be on GPU!"
                    )
                    sys.stdout.flush()
            except Exception as e:
                self.logger.warning(f"Failed to check GPU memory: {e}")
        else:
            block_gpu = block
            xp = np
            self.logger.warning(
                f"[RADIAL COMPUTE] CPU MODE: Processing block on CPU! "
                f"use_cuda={self.use_cuda}, CUDA_AVAILABLE={CUDA_AVAILABLE}"
            )
            sys.stdout.flush()

        # Direct GPU computation for block (all operations on GPU via cupy)
        # All operations will be queued to stream if provided
        self.logger.info("[RADIAL COMPUTE] STEP 2: Creating meshgrid on GPU")
        sys.stdout.flush()
        
        # DEBUG: Check GPU memory before meshgrid
        if self.use_cuda and CUDA_AVAILABLE:
            try:
                mem_before = cp.cuda.runtime.memGetInfo()
                self.logger.info(
                    f"[RADIAL COMPUTE DEBUG] Before meshgrid: GPU memory "
                    f"{mem_before[0]/1e9:.3f}GB free / {mem_before[1]/1e9:.3f}GB total"
                )
        sys.stdout.flush()
            except Exception as e:
                self.logger.warning(f"[RADIAL COMPUTE DEBUG] Failed to check GPU memory: {e}")
        
        # CRITICAL: Use cp directly for GPU operations to ensure vectors are on GPU
        if self.use_cuda and CUDA_AVAILABLE:
            x = cp.arange(block_shape[0], dtype=cp.float32)
            y = cp.arange(block_shape[1], dtype=cp.float32)
            z = cp.arange(block_shape[2], dtype=cp.float32)
            X, Y, Z = cp.meshgrid(x, y, z, indexing="ij")
            
            # CRITICAL: Verify meshgrid arrays are on GPU
            if not isinstance(X, cp.ndarray) or not isinstance(Y, cp.ndarray) or not isinstance(Z, cp.ndarray):
                raise RuntimeError(
                    f"Meshgrid not on GPU! X type: {type(X)}, Y type: {type(Y)}, Z type: {type(Z)}"
                )
            
            # Force GPU memory allocation by accessing arrays
            _ = X.device
            _ = Y.device
            _ = Z.device
        else:
            x = np.arange(block_shape[0], dtype=np.float32)
            y = np.arange(block_shape[1], dtype=np.float32)
            z = np.arange(block_shape[2], dtype=np.float32)
            X, Y, Z = np.meshgrid(x, y, z, indexing="ij")
        
        # Verify meshgrid is on GPU and check memory
        if self.use_cuda and CUDA_AVAILABLE:
            if not isinstance(X, cp.ndarray):
                raise RuntimeError(f"Meshgrid not on GPU! Type: {type(X)}")
            
            # DEBUG: Check GPU memory after meshgrid
            try:
                mem_after = cp.cuda.runtime.memGetInfo()
                meshgrid_mem = (X.nbytes + Y.nbytes + Z.nbytes) / 1e9
                meshgrid_mem_mb = (X.nbytes + Y.nbytes + Z.nbytes) / 1e6
                memory_change_mb = (mem_before[0] - mem_after[0]) / 1e6
                self.logger.info(
                    f"[RADIAL COMPUTE DEBUG] After meshgrid: GPU memory "
                    f"{mem_after[0]/1e9:.3f}GB free / {mem_after[1]/1e9:.3f}GB total, "
                    f"meshgrid size: {meshgrid_mem_mb:.2f}MB ({meshgrid_mem:.6f}GB), "
                    f"memory change: {memory_change_mb:.2f}MB, "
                    f"X shape={X.shape}, dtype={X.dtype}, device={X.device}"
                )
                sys.stdout.flush()
                
                # CRITICAL: If meshgrid memory is very small, it may not be using GPU properly
                if meshgrid_mem_mb < 1.0:  # Less than 1MB
                    self.logger.warning(
                        f"[RADIAL COMPUTE WARNING] Meshgrid memory very small ({meshgrid_mem_mb:.2f}MB)! "
                        f"Expected ~{block_shape[0]*block_shape[1]*block_shape[2]*3*4/1e6:.2f}MB for float32. "
                        f"GPU may not be storing arrays!"
                    )
                    sys.stdout.flush()
            except Exception as e:
                self.logger.warning(f"[RADIAL COMPUTE DEBUG] Failed to check GPU memory: {e}")

        self.logger.info("[RADIAL COMPUTE] STEP 3: Computing distances on GPU")
        sys.stdout.flush()
        
        # DEBUG: Check GPU memory before distances
        if self.use_cuda and CUDA_AVAILABLE:
            try:
                mem_before_dist = cp.cuda.runtime.memGetInfo()
                self.logger.info(
                    f"[RADIAL COMPUTE DEBUG] Before distances: GPU memory "
                    f"{mem_before_dist[0]/1e9:.3f}GB free / {mem_before_dist[1]/1e9:.3f}GB total"
                )
                sys.stdout.flush()
            except Exception:
                pass
        
        # CRITICAL: Use cp directly for GPU operations
        if self.use_cuda and CUDA_AVAILABLE:
            center_array = cp.array(center, dtype=cp.float32)
            distances = cp.sqrt(
            (X - center_array[0]) ** 2
            + (Y - center_array[1]) ** 2
            + (Z - center_array[2]) ** 2
        )
            
            # CRITICAL: Verify distances is on GPU
            if not isinstance(distances, cp.ndarray):
                raise RuntimeError(f"Distances not on GPU! Type: {type(distances)}")
            
            # Force GPU computation by accessing device
            _ = distances.device
        else:
            center_array = np.array(center, dtype=np.float32)
            distances = np.sqrt(
                (X - center_array[0]) ** 2
                + (Y - center_array[1]) ** 2
                + (Z - center_array[2]) ** 2
            )
            
            # DEBUG: Check GPU memory after distances
            try:
                mem_after_dist = cp.cuda.runtime.memGetInfo()
                distances_mem_gb = distances.nbytes / 1e9
                distances_mem_mb = distances.nbytes / 1e6
                memory_change_mb = (mem_before_dist[0] - mem_after_dist[0]) / 1e6
                self.logger.info(
                    f"[RADIAL COMPUTE DEBUG] After distances: GPU memory "
                    f"{mem_after_dist[0]/1e9:.3f}GB free / {mem_after_dist[1]/1e9:.3f}GB total, "
                    f"distances size: {distances_mem_mb:.2f}MB ({distances_mem_gb:.6f}GB), "
                    f"memory change: {memory_change_mb:.2f}MB, "
                    f"shape={distances.shape}, dtype={distances.dtype}, device={distances.device}"
                )
                sys.stdout.flush()
                
                # CRITICAL: If distances memory is very small, it may not be using GPU properly
                if distances_mem_mb < 1.0:  # Less than 1MB
                    self.logger.warning(
                        f"[RADIAL COMPUTE WARNING] Distances memory very small ({distances_mem_mb:.2f}MB)! "
                        f"Expected ~{np.prod(distances.shape)*4/1e6:.2f}MB for float32. "
                        f"GPU may not be storing arrays!"
                    )
                    sys.stdout.flush()
            except Exception:
                pass

        if self.use_cuda and CUDA_AVAILABLE and stream is None:
            cp.cuda.Stream.null.synchronize()
        
        self.logger.info("[RADIAL COMPUTE] STEP 4: Extracting amplitude on GPU")
        sys.stdout.flush()
        
        # DEBUG: Check GPU memory before amplitude
        if self.use_cuda and CUDA_AVAILABLE:
            try:
                mem_before_amp = cp.cuda.runtime.memGetInfo()
                self.logger.info(
                    f"[RADIAL COMPUTE DEBUG] Before amplitude: GPU memory "
                    f"{mem_before_amp[0]/1e9:.3f}GB free / {mem_before_amp[1]/1e9:.3f}GB total"
                )
                sys.stdout.flush()
            except Exception:
                pass
        
        # CRITICAL: Use cp directly for GPU operations
        if self.use_cuda and CUDA_AVAILABLE:
        if len(block_gpu.shape) == 7:
            center_phi = block_gpu.shape[3] // 2
            center_t = block_gpu.shape[6] // 2
                amplitude = cp.abs(
                block_gpu[:, :, :, center_phi, center_phi, center_phi, center_t]
            )
        else:
                amplitude = cp.abs(block_gpu)
            
            # CRITICAL: Verify amplitude is on GPU
            if not isinstance(amplitude, cp.ndarray):
                raise RuntimeError(f"Amplitude not on GPU! Type: {type(amplitude)}")
            
            # Force GPU computation
            _ = amplitude.device
        else:
            if len(block_gpu.shape) == 7:
                center_phi = block_gpu.shape[3] // 2
                center_t = block_gpu.shape[6] // 2
                amplitude = np.abs(
                    block_gpu[:, :, :, center_phi, center_phi, center_phi, center_t]
                )
            else:
                amplitude = np.abs(block_gpu)
        
            # DEBUG: Verify amplitude is on GPU and check memory
            if self.use_cuda and CUDA_AVAILABLE:
                if not isinstance(amplitude, cp.ndarray):
                    raise RuntimeError(f"Amplitude not on GPU! Type: {type(amplitude)}")
                try:
                    mem_after_amp = cp.cuda.runtime.memGetInfo()
                    amplitude_mem_gb = amplitude.nbytes / 1e9
                    amplitude_mem_mb = amplitude.nbytes / 1e6
                    memory_change_mb = (mem_before_amp[0] - mem_after_amp[0]) / 1e6
                    self.logger.info(
                        f"[RADIAL COMPUTE DEBUG] After amplitude: GPU memory "
                        f"{mem_after_amp[0]/1e9:.3f}GB free / {mem_after_amp[1]/1e9:.3f}GB total, "
                        f"amplitude size: {amplitude_mem_mb:.2f}MB ({amplitude_mem_gb:.6f}GB), "
                        f"memory change: {memory_change_mb:.2f}MB, "
                        f"shape={amplitude.shape}, dtype={amplitude.dtype}, device={amplitude.device}"
                    )
                    sys.stdout.flush()
                    
                    # CRITICAL: If amplitude memory is very small, it may not be using GPU properly
                    if amplitude_mem_mb < 1.0:  # Less than 1MB
                        self.logger.warning(
                            f"[RADIAL COMPUTE WARNING] Amplitude memory very small ({amplitude_mem_mb:.2f}MB)! "
                            f"Expected ~{np.prod(amplitude.shape)*4/1e6:.2f}MB for float32. "
                            f"GPU may not be storing arrays!"
                        )
                        sys.stdout.flush()
                except Exception:
                    pass

        if self.use_cuda and CUDA_AVAILABLE and stream is None:
            cp.cuda.Stream.null.synchronize()

        self.logger.info("[RADIAL COMPUTE] STEP 5: Computing bins on GPU")
        sys.stdout.flush()
        
        # CRITICAL: Use cp directly for GPU operations
        if self.use_cuda and CUDA_AVAILABLE:
            r_max = float(cp.max(distances))
            num_bins = min(100, max(20, int(r_max)))
            r_bins = cp.linspace(0.0, r_max, num_bins + 1)
            r_centers = (r_bins[:-1] + r_bins[1:]) / 2.0

            distances_flat = distances.ravel()
            amplitude_flat = amplitude.ravel()
            
            # CRITICAL: Verify arrays are on GPU
            if not isinstance(r_bins, cp.ndarray) or not isinstance(r_centers, cp.ndarray):
                raise RuntimeError(
                    f"Bins not on GPU! r_bins type: {type(r_bins)}, r_centers type: {type(r_centers)}"
                )
            if not isinstance(distances_flat, cp.ndarray) or not isinstance(amplitude_flat, cp.ndarray):
                raise RuntimeError(
                    f"Flattened arrays not on GPU! distances_flat type: {type(distances_flat)}, "
                    f"amplitude_flat type: {type(amplitude_flat)}"
                )
        else:
            r_max = float(np.max(distances))
            num_bins = min(100, max(20, int(r_max)))
            r_bins = np.linspace(0.0, r_max, num_bins + 1)
        r_centers = (r_bins[:-1] + r_bins[1:]) / 2.0

        distances_flat = distances.ravel()
        amplitude_flat = amplitude.ravel()

        if self.use_cuda and CUDA_AVAILABLE and stream is None:
            cp.cuda.Stream.null.synchronize()

        self.logger.info("[RADIAL COMPUTE] STEP 6: Computing bin indices on GPU")
        sys.stdout.flush()
        
        # CRITICAL: Use cp directly for GPU operations
        if self.use_cuda and CUDA_AVAILABLE:
            bin_indices = cp.searchsorted(r_bins[1:], distances_flat, side="right")
            bin_indices = cp.clip(bin_indices, 0, num_bins - 1)
            
            # CRITICAL: Convert to int32 for bincount (required by CuPy)
            bin_indices = bin_indices.astype(cp.int32)
            
            # CRITICAL: Verify bin_indices is on GPU
            if not isinstance(bin_indices, cp.ndarray):
                raise RuntimeError(f"bin_indices not on GPU! Type: {type(bin_indices)}")
        else:
            bin_indices = np.searchsorted(r_bins[1:], distances_flat, side="right")
            bin_indices = np.clip(bin_indices, 0, num_bins - 1)
            self.logger.info(
                f"[RADIAL COMPUTE DEBUG] bin_indices: shape={bin_indices.shape}, "
                f"dtype={bin_indices.dtype}, min={int(cp.min(bin_indices))}, "
                f"max={int(cp.max(bin_indices))}, num_bins={num_bins}"
            )
            sys.stdout.flush()

        # CRITICAL: Use cp directly for GPU operations
        if self.use_cuda and CUDA_AVAILABLE:
            A_radial = cp.zeros(num_bins, dtype=cp.float32)
            
            if stream is None:
                cp.cuda.Stream.null.synchronize()

            self.logger.info("[RADIAL COMPUTE] STEP 7: Computing bincount on GPU")
            sys.stdout.flush()
            
            # CRITICAL: Use int32 for bincount and ensure weights are correct dtype
            # Ensure weights are float32 for efficiency
            amplitude_flat_bincount = amplitude_flat.astype(cp.float32)
            bin_sums = cp.bincount(
                bin_indices, weights=amplitude_flat_bincount, minlength=num_bins
            )
            bin_counts = cp.bincount(bin_indices, minlength=num_bins)
            
            # CRITICAL: Verify bincount results are on GPU
            if not isinstance(bin_sums, cp.ndarray) or not isinstance(bin_counts, cp.ndarray):
                raise RuntimeError(
                    f"Bincount results not on GPU! bin_sums type: {type(bin_sums)}, "
                    f"bin_counts type: {type(bin_counts)}"
                )
            
            # DEBUG: Check results
            self.logger.info(
                f"[RADIAL COMPUTE DEBUG] bincount complete: "
                f"bin_sums shape={bin_sums.shape}, dtype={bin_sums.dtype}, "
                f"bin_counts shape={bin_counts.shape}, dtype={bin_counts.dtype}, "
                f"non_zero_bins={int(cp.count_nonzero(bin_counts))}"
            )
            sys.stdout.flush()

            if stream is None:
                cp.cuda.Stream.null.synchronize()

            self.logger.info("[RADIAL COMPUTE] STEP 8: Computing averages on GPU")
            sys.stdout.flush()
            
            # CRITICAL: Use cp directly for GPU operations
            valid_mask = bin_counts > 0
            A_radial[valid_mask] = bin_sums[valid_mask] / bin_counts[valid_mask]
            
            # CRITICAL: Verify A_radial is on GPU
            if not isinstance(A_radial, cp.ndarray):
                raise RuntimeError(f"A_radial not on GPU! Type: {type(A_radial)}")
            
            # Force GPU computation
            _ = A_radial.device
        else:
            A_radial = np.zeros(num_bins, dtype=np.float32)
            bin_sums = np.bincount(
                bin_indices, weights=amplitude_flat, minlength=num_bins
            )
            bin_counts = np.bincount(bin_indices, minlength=num_bins)
            valid_mask = bin_counts > 0
            A_radial[valid_mask] = bin_sums[valid_mask] / bin_counts[valid_mask]

        if self.use_cuda and CUDA_AVAILABLE:
            if stream is not None:
                stream.synchronize()
            else:
                cp.cuda.Stream.null.synchronize()

        self.logger.info("[RADIAL COMPUTE] STEP 9: Converting to numpy")
        sys.stdout.flush()
        
        # Convert back to numpy for return
        if self.use_cuda and CUDA_AVAILABLE:
            # Verify arrays are still on GPU before conversion
            if not isinstance(r_centers, cp.ndarray):
                self.logger.warning(f"r_centers not on GPU before conversion! Type: {type(r_centers)}")
            if not isinstance(A_radial, cp.ndarray):
                self.logger.warning(f"A_radial not on GPU before conversion! Type: {type(A_radial)}")
            
            result = {
                "r": cp.asnumpy(r_centers),
                "A": cp.asnumpy(A_radial),
            }
            self.logger.info("[RADIAL COMPUTE] COMPLETE - all operations on GPU")
            sys.stdout.flush()
            return result
        self.logger.warning(
            f"[RADIAL COMPUTE] CPU MODE: COMPLETE (CPU). "
            f"use_cuda={self.use_cuda}, CUDA_AVAILABLE={CUDA_AVAILABLE}. "
            f"This should not happen if CUDA is enabled!"
        )
        sys.stdout.flush()
        return {"r": r_centers, "A": A_radial}
