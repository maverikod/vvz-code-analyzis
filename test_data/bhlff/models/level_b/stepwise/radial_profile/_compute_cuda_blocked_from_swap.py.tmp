"""
Author: Vasiliy Zdanovskiy
email: vasilyvz@gmail.com

Radial profile computation for stepwise power law analysis.

This module implements radial profile computation methods for analyzing
the stepwise structure of phase fields, supporting both CPU and CUDA acceleration.

Theoretical Background:
    Radial profiles A(r) are computed by averaging field values over
    spherical shells centered at defects, enabling analysis of decay
    behavior and layer structure in 7D space-time.

Example:
    >>> profiler = RadialProfileComputer(use_cuda=True)
    >>> profile = profiler.compute(field, center)
"""

import numpy as np
from typing import Dict, List
import logging
import sys

# CUDA support
try:
    import cupy as cp

    CUDA_AVAILABLE = True
except ImportError:
    CUDA_AVAILABLE = False
    cp = None


class RadialProfileComputer:
    """
    Radial profile computation with CUDA acceleration.

    Physical Meaning:
        Computes radial profiles by averaging field values over spherical
        shells, providing the basis for analyzing decay behavior and
        layer structure in the phase field.

    Mathematical Foundation:
        For a field a(x), the radial profile A(r) is computed as:
        A(r) = (1/V_r) âˆ«_{|x-c|=r} |a(x)| dS
        where V_r is the volume of the spherical shell at radius r.
    """

    def __init__(self, use_cuda: bool = True, gpu_memory_ratio: float = 0.8):
        """
        Initialize radial profile computer.

        Physical Meaning:
            Sets up computer with CUDA acceleration for efficient
            computation of radial profiles in 7D phase fields.

        Args:
            use_cuda (bool): Whether to use CUDA acceleration.
            gpu_memory_ratio (float): GPU memory utilization ratio (0-1).
        """
        self.use_cuda = use_cuda and CUDA_AVAILABLE
        self.gpu_memory_ratio = gpu_memory_ratio
        self.logger = logging.getLogger(__name__)

        if self.use_cuda:
            self.xp = cp
            try:
                from ....utils.cuda_utils import get_global_backend

                self.backend = get_global_backend()
            except ImportError:
                self.backend = None
        else:
            self.xp = np
            self.backend = None

    def compute(self, field: np.ndarray, center: List[float]) -> Dict[str, np.ndarray]:
        """
        Compute radial profile of the field with automatic swap/blocking.

        Physical Meaning:
            Computes the radial profile A(r) by averaging the field
            over spherical shells centered at the defect. Automatically
            uses FieldArray for transparent swap and block processing
            when field size exceeds GPU memory.

        Args:
            field (np.ndarray): 3D or 7D field array (will be wrapped in FieldArray
                for automatic swap management if needed).
            center (List[float]): Center coordinates [x, y, z].

        Returns:
            Dict[str, np.ndarray]: Radial profile with 'r' and 'A' arrays.
        """
        import sys
        field_size_mb = field.nbytes / (1024**2) if hasattr(field, 'nbytes') else 0
        self.logger.info(
            f"[RADIAL PROFILE] compute: START - field shape={field.shape}, "
            f"size={field_size_mb:.2f}MB, center={center}, use_cuda={self.use_cuda}"
        )
        sys.stdout.flush()
        sys.stderr.flush()
        
        # Automatically wrap field in FieldArray for transparent swap management
        from bhlff.core.arrays.field_array import FieldArray

        self.logger.info(f"[RADIAL PROFILE] STEP 1: Wrapping field in FieldArray if needed...")
        sys.stdout.flush()
        sys.stderr.flush()
        if not isinstance(field, FieldArray):
            # Wrap in FieldArray - it will automatically use swap if field is large
            field_wrapped = FieldArray(array=field)
            self.logger.info(f"[RADIAL PROFILE] STEP 1 COMPLETE: Field wrapped in FieldArray")
        else:
            field_wrapped = field
            self.logger.info(f"[RADIAL PROFILE] STEP 1 COMPLETE: Field already FieldArray")
        sys.stdout.flush()
        sys.stderr.flush()

        # Extract array (may be memory-mapped if swapped)
        self.logger.info(f"[RADIAL PROFILE] STEP 2: Extracting array...")
        sys.stdout.flush()
        sys.stderr.flush()
        field_array = field_wrapped.array
        is_swapped = isinstance(field_array, np.memmap)
        self.logger.info(
            f"[RADIAL PROFILE] STEP 2 COMPLETE: Array extracted, shape={field_array.shape if field_array is not None else None}, "
            f"is_swapped={is_swapped}, type={type(field_array).__name__}"
        )
        sys.stdout.flush()
        sys.stderr.flush()

        # CRITICAL: For 7D fields, ALWAYS use window-based processing to maximize GPU utilization
        # This ensures large windows (80% GPU memory) are processed entirely on GPU
        if self.use_cuda and len(field_array.shape) == 7:
            self.logger.info(
                f"[RADIAL PROFILE] STEP 3: 7D field detected, using window-based GPU processing "
                f"(is_swapped={is_swapped})"
            )
            sys.stdout.flush()
            return self._compute_cuda_with_swap(field_array, center, field_array.shape[:3])

        if self.use_cuda:
            self.logger.info(
                f"[RADIAL PROFILE] GPU MODE: Using _compute_cuda for non-7D field. "
                f"Field shape={field_array.shape}, size={field_array.nbytes/1e9:.3f}GB"
            )
            sys.stdout.flush()
            return self._compute_cuda(field_array, center)
        else:
            self.logger.warning(
                f"[RADIAL PROFILE] CPU MODE: use_cuda=False, using _compute_cpu. "
                f"Field shape={field_array.shape}, size={field_array.nbytes/1e9:.3f}GB"
            )
            sys.stdout.flush()
            return self._compute_cpu(field_array, center)

    def compute_substrate(
        self, substrate: np.ndarray, center: List[float]
    ) -> Dict[str, np.ndarray]:
        """
        Compute radial profile of substrate transparency.

        Physical Meaning:
            Computes the radial profile T(r) by averaging the substrate
            transparency over spherical shells centered at the defect
            using vectorized operations for efficiency.

        Args:
            substrate (np.ndarray): 7D substrate field.
            center (List[float]): Center coordinates [x, y, z].

        Returns:
            Dict[str, np.ndarray]: Radial profile with 'r' and 'A' arrays.
        """
        # Always use CUDA if enabled, converting numpy arrays to cupy
        use_cuda_here = self.use_cuda
        xp = self.xp if use_cuda_here else np
        
        # Convert numpy array to cupy if CUDA is enabled
        if use_cuda_here and isinstance(substrate, np.ndarray):
            substrate = xp.asarray(substrate)

        if len(substrate.shape) == 7:
            shape = substrate.shape[:3]
        else:
            shape = substrate.shape[:3]

        x = xp.arange(shape[0], dtype=xp.float32)
        y = xp.arange(shape[1], dtype=xp.float32)
        z = xp.arange(shape[2], dtype=xp.float32)
        X, Y, Z = xp.meshgrid(x, y, z, indexing="ij")

        center_array = xp.array(center, dtype=xp.float32)
        distances = xp.sqrt(
            (X - center_array[0]) ** 2
            + (Y - center_array[1]) ** 2
            + (Z - center_array[2]) ** 2
        )

        if len(substrate.shape) == 7:
            center_phi = substrate.shape[3] // 2
            center_t = substrate.shape[6] // 2
            transparency = xp.abs(
                substrate[:, :, :, center_phi, center_phi, center_phi, center_t]
            )
        else:
            transparency = xp.abs(substrate)

        r_max = float(xp.max(distances))
        num_bins = max(20, min(100, int(r_max * 10)))
        r_bins = xp.linspace(0.0, r_max, num_bins + 1)
        r_centers = (r_bins[:-1] + r_bins[1:]) / 2.0

        distances_flat = distances.ravel()
        transparency_flat = transparency.ravel()
        bin_indices = xp.searchsorted(r_bins[1:], distances_flat, side="right")
        bin_indices = xp.clip(bin_indices, 0, num_bins - 1)

        T_radial = xp.zeros(num_bins, dtype=xp.float32)
        if hasattr(xp, "bincount"):
            bin_sums = xp.bincount(
                bin_indices, weights=transparency_flat, minlength=num_bins
            )
            bin_counts = xp.bincount(bin_indices, minlength=num_bins)
            valid_mask = bin_counts > 0
            T_radial[valid_mask] = bin_sums[valid_mask] / bin_counts[valid_mask]
        else:
            for i in range(num_bins):
                mask = bin_indices == i
                if xp.any(mask):
                    T_radial[i] = xp.mean(transparency_flat[mask])

        # Always convert back to numpy for return
        if use_cuda_here:
            T_radial = cp.asnumpy(T_radial)
            r_centers = cp.asnumpy(r_centers)

        return {"r": r_centers, "A": T_radial}


    def _compute_cuda_blocked_from_swap(
        self, field: np.ndarray, center: List[float], shape: tuple
    ) -> Dict[str, np.ndarray]:
        """
        Compute radial profile using window-based processing for maximum GPU utilization.

        Physical Meaning:
            Processes field using window-based approach:
            - Forms windows equal to 80% of GPU memory
            - Processes each window entirely on GPU
            - Aggregates results from all windows
            - Works with memory-mapped arrays transparently
            
        This approach maximizes GPU utilization by processing large chunks
        instead of small blocks, similar to FFT window-based processing.

        Args:
            field (np.ndarray): Field array (may be memory-mapped).
            center (List[float]): Center coordinates.
            shape (tuple): Spatial shape.

        Returns:
            Dict[str, np.ndarray]: Radial profile.
        """
        if not CUDA_AVAILABLE or not self.use_cuda:
            self.logger.warning(
                f"[RADIAL PROFILE] CPU MODE: CUDA not available or disabled in _compute_cuda_with_swap. "
                f"CUDA_AVAILABLE={CUDA_AVAILABLE}, use_cuda={self.use_cuda}"
            )
            sys.stdout.flush()
            return self._compute_cpu(field, center)

        try:
            from ....utils.cuda_utils import calculate_optimal_window_memory
            from itertools import product
            
            self.logger.info(
                f"Processing field with window-based approach: shape={field.shape}, "
                f"CUDA={CUDA_AVAILABLE}, use_cuda={self.use_cuda}"
            )
            
            # CRITICAL: Use same window calculation as FFT solver for consistency
            # Radial profile needs ~5x memory: meshgrid (3x), distances (1x), amplitude (1x), temp arrays (2x)
            overhead_factor = 5.0
            
            # Calculate maximum window size using centralized utility function (same as FFT solver)
            max_window_elements, actual_usage_gb, actual_usage_pct = calculate_optimal_window_memory(
                gpu_memory_ratio=self.gpu_memory_ratio,
                overhead_factor=overhead_factor,
                logger=self.logger,
            )
            
            self.logger.info(
                f"Radial profile window calculation: max_window={max_window_elements/1e6:.1f}M elements, "
                f"expected usage={actual_usage_gb:.3f}GB ({actual_usage_pct:.1f}% of total GPU memory)"
            )
            sys.stdout.flush()
            
            # Calculate window size per dimension (same logic as FFT solver)
            field_elements = np.prod(field.shape)
            
            # If field fits in window, process as single window
            if field_elements <= max_window_elements:
                self.logger.info(f"Field fits in single window, processing entirely on GPU")
                return self._compute_cuda_single_block(field, center)

            # For 7D fields, calculate window size for spatial dimensions
            # Keep phase/temporal dimensions full (same approach as FFT solver)
            N_x, N_y, N_z = shape
            
            if len(field.shape) == 7:
                # Calculate window size for spatial dimensions
                # Keep phase/temporal dimensions full
                phase_temporal_size = np.prod(field.shape[3:])
                max_spatial_elements = max_window_elements // phase_temporal_size
                
                # Calculate window size per spatial dimension
                spatial_dims = field.shape[:3]
                elements_per_spatial_dim = int(max_spatial_elements ** (1.0 / 3.0))
                
                # Window size: ensure at least 32 per dimension for GPU efficiency (same as FFT solver)
                window_size = tuple(
                    max(32, min(elements_per_spatial_dim, dim))
                    for dim in spatial_dims
                ) + field.shape[3:]  # Keep phase/temporal dimensions full
            else:
                # 3D field: use all dimensions
                elements_per_dim = int(max_window_elements ** (1.0 / 3.0))
                window_size = tuple(
                    max(32, min(elements_per_dim, dim))
                    for dim in field.shape
                )

            window_elements = np.prod(window_size)
            window_size_mb = (window_elements * 16) / (1024**2)  # complex128 = 16 bytes
            
            self.logger.info(
                f"Window size: {window_size} = {window_elements/1e6:.1f}M elements = {window_size_mb:.2f}MB"
            )
            sys.stdout.flush()
            
            # Log actual GPU memory status
            try:
                mem_info = cp.cuda.runtime.memGetInfo()
                free_mem_gb = mem_info[0] / 1e9
                total_mem_gb = mem_info[1] / 1e9
                used_mem_gb = total_mem_gb - free_mem_gb
                self.logger.info(
                    f"GPU memory status: {used_mem_gb:.2f}GB used / {total_mem_gb:.2f}GB total "
                    f"({used_mem_gb/total_mem_gb*100:.1f}% used), "
                    f"{free_mem_gb:.2f}GB free"
                )
            except Exception as e:
                self.logger.warning(f"Failed to get GPU memory status: {e}")

            # Calculate number of windows needed
            num_windows_x = (N_x + window_size[0] - 1) // window_size[0]
            num_windows_y = (N_y + window_size[1] - 1) // window_size[1]
            num_windows_z = (N_z + window_size[2] - 1) // window_size[2]
            total_windows = num_windows_x * num_windows_y * num_windows_z
            
            self.logger.info(
                f"Total windows: {total_windows} ({num_windows_x}x{num_windows_y}x{num_windows_z})"
            )

            # Aggregate radial profiles from all windows
            all_r = []
            all_A = []

            # Process windows: each window is processed entirely on GPU
            # For large windows, can process sub-windows in parallel via CUDA streams
            window_idx = 0
            for wx in range(num_windows_x):
                for wy in range(num_windows_y):
                    for wz in range(num_windows_z):
                        # Calculate window boundaries
                        x_start = wx * window_size[0]
                        x_end = min(x_start + window_size[0], N_x)
                        y_start = wy * window_size[1]
                        y_end = min(y_start + window_size[1], N_y)
                        z_start = wz * window_size[2]
                        z_end = min(z_start + window_size[2], N_z)
                        
                        # Extract window
                        if len(field.shape) == 7:
                            window_slice = field[x_start:x_end, y_start:y_end, z_start:z_end, :, :, :, :]
                        else:
                            window_slice = field[x_start:x_end, y_start:y_end, z_start:z_end]

                        # CRITICAL: Copy to numpy array if memmap (same as FFT solver)
                        # memmap slices are views, need explicit copy for GPU transfer
                        if isinstance(window_slice, np.memmap):
                            self.logger.info(
                                f"[RADIAL WINDOW {window_idx+1}/{total_windows}] Copying memmap slice to numpy array, "
                                f"size: {window_slice.nbytes/1e9:.3f}GB..."
                            )
                            sys.stdout.flush()
                            window_cpu = np.array(window_slice, copy=True)
                            self.logger.info(
                                f"[RADIAL WINDOW {window_idx+1}/{total_windows}] Copy complete, "
                                f"type: {type(window_cpu).__name__}"
                            )
                            sys.stdout.flush()
                        else:
                            window_cpu = window_slice

                        # Adjust center for this window
                        window_center = [
                            center[0] - x_start,
                            center[1] - y_start,
                            center[2] - z_start,
                        ]

                        # Process window entirely on GPU with parallel sub-window processing
                        try:
                            # STEP-BY-STEP LOGGING: Track every window operation
                            window_size_mb = window_cpu.nbytes / (1024**2)
                            self.logger.info(
                                f"[RADIAL WINDOW {window_idx+1}/{total_windows}] START: "
                                f"({x_start}:{x_end}, {y_start}:{y_end}, {z_start}:{z_end}), "
                                f"window size: {window_size_mb:.2f}MB ({window_cpu.nbytes/1e9:.3f}GB)"
                            )
                            sys.stdout.flush()
                            sys.stderr.flush()
                            
                            # Check GPU memory before processing
                            mem_info_before = cp.cuda.runtime.memGetInfo()
                            free_mem_before = mem_info_before[0] / 1e9
                            total_mem = mem_info_before[1] / 1e9
                            used_mem_before = (total_mem - free_mem_before)
                            
                            # Transfer window to GPU first
                            # CRITICAL: Ensure data is actually transferred to GPU
                            self.logger.info(
                                f"[RADIAL WINDOW {window_idx+1}/{total_windows}] STEP 1: "
                                f"Transferring to GPU, GPU memory: {used_mem_before:.2f}GB used / "
                                f"{total_mem:.2f}GB total ({used_mem_before/total_mem*100:.1f}% used), "
                                f"{free_mem_before:.2f}GB free"
                            )
                            sys.stdout.flush()
                            
                            if self.backend is not None:
                                window_gpu = self.backend.array(window_cpu)
                            else:
                                window_gpu = cp.asarray(window_cpu)
                            
                            # CRITICAL: Synchronize to ensure transfer completes
                            cp.cuda.Stream.null.synchronize()
                            
                            # CRITICAL: Verify window is actually on GPU
                            if not isinstance(window_gpu, cp.ndarray):
                                raise RuntimeError(
                                    f"Window not on GPU! Type: {type(window_gpu)}, "
                                    f"expected cp.ndarray"
                                )
                            
                            # Check GPU memory after transfer
                            mem_info_after = cp.cuda.runtime.memGetInfo()
                            free_mem_after = mem_info_after[0] / 1e9
                            used_mem_after = (total_mem - free_mem_after)
                            window_mem_used = used_mem_after - used_mem_before
                            
                            # Verify GPU memory actually increased
                            if window_mem_used < 0.001:  # Less than 1MB - suspicious
                                self.logger.warning(
                                    f"[RADIAL WINDOW {window_idx+1}/{total_windows}] WARNING: "
                                    f"GPU memory usage very low ({window_mem_used:.6f}GB) after transfer! "
                                    f"Window size: {window_cpu.nbytes/1e9:.3f}GB. "
                                    f"Data may not be on GPU!"
                                )
                            
                            self.logger.info(
                                f"[RADIAL WINDOW {window_idx+1}/{total_windows}] STEP 2: "
                                f"Window on GPU, shape={window_gpu.shape}, type={type(window_gpu).__name__}, "
                                f"GPU memory: {used_mem_after:.2f}GB used ({used_mem_after/total_mem*100:.1f}%), "
                                f"window used {window_mem_used:.3f}GB, "
                                f"window size: {window_cpu.nbytes/1e9:.3f}GB"
                            )
                            sys.stdout.flush()
                            
                            # CRITICAL: Check if window is large enough to benefit from parallel sub-window processing
                            # For windows > 200MB, split into 3-4 sub-windows and process in parallel via streams
                            # Split by temporal dimension (dimension 6) for 7D fields
                            use_parallel_subwindows = (
                                CUDA_AVAILABLE and 
                                window_size_mb > 200 and 
                                len(window_gpu.shape) == 7 and
                                window_gpu.shape[6] >= 2  # Need at least 2 time slices to split
                            )
                            
                            if use_parallel_subwindows:
                                # Calculate optimal number of sub-windows (3-4 streams)
                                # Each sub-window should be at least 100MB for GPU efficiency
                                min_subwindow_mb = 100
                                max_subwindows = max(2, int(window_size_mb / min_subwindow_mb))
                                num_subwindows = min(max_subwindows, window_gpu.shape[6], 4)  # Use up to 4 streams
                                
                                # Only use parallel processing if we get at least 2 sub-windows
                                if num_subwindows >= 2:
                                    subwindow_size_t = (window_gpu.shape[6] + num_subwindows - 1) // num_subwindows
                                    
                                    # Create CUDA streams for parallel processing
                                    streams = [cp.cuda.Stream() for _ in range(num_subwindows)]
                                    subwindow_results = []
                                    
                                    self.logger.info(
                                        f"[RADIAL WINDOW {window_idx+1}/{total_windows}] Splitting into {num_subwindows} "
                                        f"sub-windows for parallel processing via CUDA streams"
                                    )
                                    sys.stdout.flush()
                                    
                                    # Process sub-windows in parallel via streams
                                    for stream_idx, stream in enumerate(streams):
                                        t_start = stream_idx * subwindow_size_t
                                        t_end = min(t_start + subwindow_size_t, window_gpu.shape[6])
                                        
                                        if t_start >= window_gpu.shape[6]:
                                            break
                                        
                                        # Extract sub-window (full spatial + phase, slice of time)
                                        subwindow_gpu = window_gpu[:, :, :, :, :, :, t_start:t_end]
                                        
                                        # Process sub-window in stream (parallel)
                                        with stream:
                                            subwindow_profile = self._compute_cuda_single_block(
                                                subwindow_gpu, window_center
                                            )
                                            # Keep result on GPU, store for later aggregation
                                            subwindow_results.append(subwindow_profile)
                                    
                                    # Synchronize all streams
                                    self.logger.info(
                                        f"[RADIAL WINDOW {window_idx+1}/{total_windows}] Synchronizing {num_subwindows} streams"
                                    )
                                    sys.stdout.flush()
                                    for stream in streams:
                                        stream.synchronize()
                                    
                                    # Aggregate sub-window results
                                    # Combine r and A arrays from all sub-windows
                                    window_profile = self._aggregate_subwindow_profiles(subwindow_results)
                                    
                                    self.logger.info(
                                        f"[RADIAL WINDOW {window_idx+1}/{total_windows}] Parallel processing complete: "
                                        f"{num_subwindows} sub-windows processed"
                                    )
                                    sys.stdout.flush()
                                else:
                                    # Window too small for parallel processing, process normally
                                    window_profile = self._compute_cuda_single_block(
                                        window_gpu, window_center
                                    )
                            else:
                                # Window too small or not 7D, process normally
                            window_profile = self._compute_cuda_single_block(
                                window_gpu, window_center
                            )
                            
                            # Check GPU memory after computation
                            mem_info_after_comp = cp.cuda.runtime.memGetInfo()
                            used_mem_after_comp = (total_mem - mem_info_after_comp[0] / 1e9)
                            
                            self.logger.info(
                                f"[RADIAL WINDOW {window_idx+1}/{total_windows}] STEP 3: "
                                f"Computation complete, GPU memory: {used_mem_after_comp:.2f}GB used "
                                f"({used_mem_after_comp/total_mem*100:.1f}%), "
                                f"peak during computation: {used_mem_after_comp:.2f}GB"
                            )
                            sys.stdout.flush()
                            
                            all_r.append(window_profile["r"])
                            all_A.append(window_profile["A"])
                            
                            # CRITICAL: Free GPU memory immediately after processing
                            # This prevents memory accumulation that could cause hard reset
                            del window_gpu
                            del window_cpu  # Also free CPU copy
                            
                            # Force memory pool cleanup
                            cp.get_default_memory_pool().free_all_blocks()
                            cp.get_default_pinned_memory_pool().free_all_blocks()
                            
                            # Synchronize to ensure cleanup completes
                            cp.cuda.Stream.null.synchronize()
                            
                            # Check memory after cleanup
                            mem_after = cp.cuda.runtime.memGetInfo()[0] / 1e9
                            
                            self.logger.info(
                                f"[RADIAL WINDOW {window_idx+1}/{total_windows}] STEP 4 COMPLETE: "
                                f"Memory freed, free_mem={mem_after:.2f}GB"
                            )
                            sys.stdout.flush()
                            
                        except Exception as e:
                            # Fallback to CPU for this window
                            self.logger.warning(
                                f"GPU window processing failed: {e}, using CPU for window"
                            )
                            window_profile = self._compute_cpu(window_cpu, window_center)
                            all_r.append(window_profile["r"])
                            all_A.append(window_profile["A"])

                        window_idx += 1

            # Aggregate profiles from all windows
            if all_r:
                # Use first window's r bins as reference
                r_ref = all_r[0]
                A_combined = np.zeros_like(r_ref)

                for r, A in zip(all_r, all_A):
                    # Interpolate A to r_ref if needed
                    if len(r) == len(r_ref) and np.allclose(r, r_ref):
                        A_combined += A
                    else:
                        # Interpolate
                        from scipy.interpolate import interp1d

                        interp = interp1d(
                            r, A, kind="linear", fill_value=0.0, bounds_error=False
                        )
                        A_combined += interp(r_ref)

                # Average
                A_combined /= len(all_A)

                self.logger.info(
                    f"Window-based processing completed: {total_windows} windows processed"
                )
                return {"r": r_ref, "A": A_combined}
            else:
                # Fallback to CPU if no windows processed
                self.logger.warning(
                    f"[RADIAL PROFILE] CPU MODE: No windows processed, falling back to CPU. "
                    f"Field shape={field.shape}, total_windows={total_windows}"
                )
                sys.stdout.flush()
                return self._compute_cpu(field, center)

        except Exception as e:
            self.logger.warning(
                f"[RADIAL PROFILE] CPU MODE: GPU window-based processing failed: {e}, falling back to CPU"
            )
            import traceback
            self.logger.debug(traceback.format_exc())
            sys.stdout.flush()
            return self._compute_cpu(field, center)
    